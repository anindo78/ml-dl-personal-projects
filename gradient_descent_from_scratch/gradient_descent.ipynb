{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278d4746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and renamed. Shape: (8000, 12)\n",
      "   user_id  gender  age country subscription_type  listening_time  \\\n",
      "0        1  Female   54      CA              Free              26   \n",
      "1        2   Other   33      DE            Family             141   \n",
      "2        3    Male   38      AU           Premium             199   \n",
      "3        4  Female   22      CA           Student              36   \n",
      "4        5   Other   29      US            Family             250   \n",
      "\n",
      "   songs_played_per_day  skip_rate device_type  ads_listened_per_week  \\\n",
      "0                    23       0.20     Desktop                     31   \n",
      "1                    62       0.34         Web                      0   \n",
      "2                    38       0.04      Mobile                      0   \n",
      "3                     2       0.31      Mobile                      0   \n",
      "4                    57       0.36      Mobile                      0   \n",
      "\n",
      "   offline_listening  is_churned  \n",
      "0                  0           1  \n",
      "1                  1           0  \n",
      "2                  1           1  \n",
      "3                  1           0  \n",
      "4                  1           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('spotify_churn_dataset.csv')\n",
    "print(\"Dataset loaded and renamed. Shape:\", df.shape)\n",
    "print(df.head())  # Inspect first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f4a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Shape: (8000, 12)\n",
      "Columns: ['user_id', 'gender', 'age', 'country', 'subscription_type', 'listening_time', 'songs_played_per_day', 'skip_rate', 'device_type', 'ads_listened_per_week', 'offline_listening', 'is_churned']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset loaded. Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f038ec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset shape: (8000, 21)\n",
      "        age  listening_time  songs_played_per_day  skip_rate  \\\n",
      "0  0.883721        0.055363              0.224490   0.333333   \n",
      "1  0.395349        0.453287              0.622449   0.566667   \n",
      "2  0.511628        0.653979              0.377551   0.066667   \n",
      "3  0.139535        0.089965              0.010204   0.516667   \n",
      "4  0.302326        0.830450              0.571429   0.600000   \n",
      "\n",
      "   ads_listened_per_week  offline_listening  is_churned  gender_Male  \\\n",
      "0               0.632653                  0           1        False   \n",
      "1               0.000000                  1           0        False   \n",
      "2               0.000000                  1           1         True   \n",
      "3               0.000000                  1           0        False   \n",
      "4               0.000000                  1           1        False   \n",
      "\n",
      "   gender_Other  country_CA  ...  country_FR  country_IN  country_PK  \\\n",
      "0         False        True  ...       False       False       False   \n",
      "1          True       False  ...       False       False       False   \n",
      "2         False       False  ...       False       False       False   \n",
      "3         False        True  ...       False       False       False   \n",
      "4          True       False  ...       False       False       False   \n",
      "\n",
      "   country_UK  country_US  subscription_type_Free  subscription_type_Premium  \\\n",
      "0       False       False                    True                      False   \n",
      "1       False       False                   False                      False   \n",
      "2       False       False                   False                       True   \n",
      "3       False       False                   False                      False   \n",
      "4       False        True                   False                      False   \n",
      "\n",
      "   subscription_type_Student  device_type_Mobile  device_type_Web  \n",
      "0                      False               False            False  \n",
      "1                      False               False             True  \n",
      "2                      False                True            False  \n",
      "3                       True                True            False  \n",
      "4                      False                True            False  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary column\n",
    "# df = df.drop(columns=['user_id'])\n",
    "\n",
    "# Handle missing values: Drop rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical variables (one-hot encoding)\n",
    "categorical_cols = ['gender', 'country', 'subscription_type', 'device_type']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalize numeric features (min-max scaling)\n",
    "numeric_cols = ['age', 'listening_time', 'songs_played_per_day', 'skip_rate', 'ads_listened_per_week']\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "\n",
    "print(\"Preprocessed dataset shape:\", df_encoded.shape)\n",
    "print(df_encoded.head())  # Preview first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12b1624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (8000, 20)\n",
      "Target (y) shape: (8000,)\n",
      "Sample features:\n",
      "         age  listening_time  songs_played_per_day  skip_rate  \\\n",
      "0  0.883721        0.055363              0.224490   0.333333   \n",
      "1  0.395349        0.453287              0.622449   0.566667   \n",
      "2  0.511628        0.653979              0.377551   0.066667   \n",
      "3  0.139535        0.089965              0.010204   0.516667   \n",
      "4  0.302326        0.830450              0.571429   0.600000   \n",
      "\n",
      "   ads_listened_per_week  offline_listening  gender_Male  gender_Other  \\\n",
      "0               0.632653                  0        False         False   \n",
      "1               0.000000                  1        False          True   \n",
      "2               0.000000                  1         True         False   \n",
      "3               0.000000                  1        False         False   \n",
      "4               0.000000                  1        False          True   \n",
      "\n",
      "   country_CA  country_DE  country_FR  country_IN  country_PK  country_UK  \\\n",
      "0        True       False       False       False       False       False   \n",
      "1       False        True       False       False       False       False   \n",
      "2       False       False       False       False       False       False   \n",
      "3        True       False       False       False       False       False   \n",
      "4       False       False       False       False       False       False   \n",
      "\n",
      "   country_US  subscription_type_Free  subscription_type_Premium  \\\n",
      "0       False                    True                      False   \n",
      "1       False                   False                      False   \n",
      "2       False                   False                       True   \n",
      "3       False                   False                      False   \n",
      "4        True                   False                      False   \n",
      "\n",
      "   subscription_type_Student  device_type_Mobile  device_type_Web  \n",
      "0                      False               False            False  \n",
      "1                      False               False             True  \n",
      "2                      False                True            False  \n",
      "3                       True                True            False  \n",
      "4                      False                True            False  \n",
      "Sample target:\n",
      " 0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: is_churned, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select features (X)\n",
    "X = df_encoded.drop(columns=['is_churned'])\n",
    "\n",
    "# Define target (y)\n",
    "y = df_encoded['is_churned']\n",
    "\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Target (y) shape:\", y.shape)\n",
    "print(\"Sample features:\\n\", X.head())\n",
    "print(\"Sample target:\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961c96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some utility functions first\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid function with numerical stability.\"\"\"\n",
    "    return 1 / (1 + math.exp(-z)) if z >= 0 else math.exp(z) / (1 + math.exp(z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n",
    "    return a * (1-a)\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Compute the dot product of two vectors.\"\"\"\n",
    "    return sum(x*y for x, y in zip(a, b))\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"Multiply two matrices A and B.\"\"\"\n",
    "    result = [[0]*len(B[0]) for _ in range(len(A))]\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B[0])):\n",
    "            result[i][j] = dot_product(A[i], [B[k][j] for k in range(len(B))])\n",
    "    return result\n",
    "\n",
    "\n",
    "def transpose(M):\n",
    "    \"\"\"Transpose a matrix M.\"\"\"\n",
    "    return list(map(list, zip(*M)))\n",
    "\n",
    "def add_bias(X):\n",
    "    \"\"\"Add a bias term (column of ones) to the feature matrix X.\"\"\"\n",
    "    return [[1] + row for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6c5edd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Batch GD:\n",
      "Epoch 0, Loss: 0.8604389948152916\n",
      "Epoch 0, Loss: 0.8604389948152916\n",
      "Epoch 100, Loss: 0.5992122970362735\n",
      "Epoch 100, Loss: 0.5992122970362735\n",
      "Epoch 200, Loss: 0.5760564084416907\n",
      "Epoch 200, Loss: 0.5760564084416907\n",
      "Epoch 300, Loss: 0.5729917899532306\n",
      "Epoch 300, Loss: 0.5729917899532306\n",
      "Epoch 400, Loss: 0.5724798575312442\n",
      "Epoch 400, Loss: 0.5724798575312442\n",
      "Epoch 500, Loss: 0.5723798388506027\n",
      "Epoch 500, Loss: 0.5723798388506027\n",
      "Epoch 600, Loss: 0.5723512447288622\n",
      "Epoch 600, Loss: 0.5723512447288622\n",
      "Epoch 700, Loss: 0.5723346499311971\n",
      "Epoch 700, Loss: 0.5723346499311971\n",
      "Epoch 800, Loss: 0.5723197797636448\n",
      "Epoch 800, Loss: 0.5723197797636448\n",
      "Epoch 900, Loss: 0.5723051029838688\n",
      "Epoch 900, Loss: 0.5723051029838688\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 74.11%\n",
      "AUC-ROC Score: 0.5099\n",
      "AUC-ROC Score: 0.5099\n"
     ]
    }
   ],
   "source": [
    "# create Nueral Network class\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights with small random values\n",
    "        self.learning_rate = learning_rate\n",
    "        # Weights from input to hidden layer\n",
    "        self.W1 = [[random.uniform(-0.5, 0.5) for _ in range(hidden_size)] for _ in range(input_size + 1)]  # +1 for bias\n",
    "\n",
    "        # Weights from hidden to output layer\n",
    "        self.W2 = [[random.uniform(-0.5, 0.5) for _ in range(output_size)] for _ in range(hidden_size + 1)]  # +1 for bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward propagation.\"\"\"\n",
    "        self.A1 = add_bias(X)  # Add bias to input layer\n",
    "\n",
    "        # from input to hidden layer -> multiply input with weights. (Num Samples * (Features + 1)) * ((Features + 1) * Hidden Neurons) = (Num Samples * Hidden Neurons)\n",
    "        self.Z2 = matrix_multiply(self.A1, self.W1)\n",
    "\n",
    "        # Input layer vector converted to hidden layer activations using sigmoid. (Num Samples * Hidden Neurons) matrix but has the probabilities\n",
    "        self.A2 = [[sigmoid(z) for z in row] for row in self.Z2]\n",
    "\n",
    "        self.A2 = add_bias(self.A2)  # Add bias to hidden layer. (Num Samples * (Hidden Neurons + 1))\n",
    "\n",
    "        # from hidden to output layer -> multiply hidden layer with weights. (Num Samples * (Hidden Neurons + 1)) * ((Hidden Neurons + 1) * Output Neurons) = (Num Samples * Output Neurons)\n",
    "        self.Z3 = matrix_multiply(self.A2, self.W2)\n",
    "\n",
    "        # Hidden layer vector converted to output layer activations using sigmoid. (Num Samples * Output Neurons) matrix but has the probabilities\n",
    "        self.A3 = [[sigmoid(z) for z in row] for row in self.Z3]\n",
    "        return self.A3\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Perform backward propagation and update weights.\"\"\"\n",
    "\n",
    "        m = len(y)\n",
    "        # Output layer target values as matrix. Num Samples * Output Neurons\n",
    "        y_matrix = [[val] for val in y]  \n",
    "\n",
    "        # Calculate output layer error. Self.A3 is the predicted output from forward pass. y_matrix is the actual output\n",
    "        # sigmoid_derivative is used to compute the gradient\n",
    "        # ((Num Samples * Output Neurons) - (Num Samples * Output Neurons)) * (Num Samples * Output Neurons) = (Num Samples * Output Neurons)\n",
    "        output_errors = [[(self.A3[i][0] - y_matrix[i][0]) * sigmoid_derivative(self.A3[i][0])] for i in range(m)]\n",
    "\n",
    "        # Calculate hidden layer error by multiplying errors with transposed weights. Exclude bias weight of W2 when backpropagating\n",
    "        # This yields a (m x hidden_neurons) matrix\n",
    "        hidden_errors = matrix_multiply(output_errors, transpose(self.W2[1:]))  # Exclude bias weight\n",
    "\n",
    "        # Apply sigmoid derivative to hidden layer errors element-wise and keep shape (m x hidden_neurons).\n",
    "        # Note: self.A2 includes a bias column at index 0, so hidden activations for neuron j are at self.A2[i][j+1].\n",
    "        hidden_errors = [[hidden_errors[i][j] * sigmoid_derivative(self.A2[i][j+1]) for j in range(len(hidden_errors[0]))] for i in range(m)]\n",
    "\n",
    "        # Update weights\n",
    "       \n",
    "        A2_T = transpose(self.A2)  # convert them to (Hidden Neurons + 1) * Num Samples matrix\n",
    "        A1_T = transpose(self.A1)  # convert them to (Features + 1) * Num Samples matrix\n",
    "\n",
    "        # Compute gradients\n",
    "        # For Output layer: (Hidden Neurons + 1) * Num Samples) * (Num Samples * Output Neurons) = ((Hidden Neurons + 1) * Output Neurons)\n",
    "        W2_gradients = matrix_multiply(A2_T, output_errors)\n",
    "\n",
    "        # For Hidden layer: (Features + 1) * Num Samples) * (Num Samples * (Hidden Neurons + 1)) = ((Features + 1) * (Hidden Neurons + 1))\n",
    "        W1_gradients = matrix_multiply(A1_T, hidden_errors)\n",
    "\n",
    "        # Gradient descent step\n",
    "        # Update weights by subtracting the gradients scaled by learning rate and averaged over all samples\n",
    "        self.W2 = [[self.W2[i][j] - self.learning_rate * W2_gradients[i][j] / m for j in range(len(self.W2[0]))] for i in range(len(self.W2))]\n",
    "\n",
    "        self.W1 = [[self.W1[i][j] - self.learning_rate * W1_gradients[i][j] / m for j in range(len(self.W1[0]))] for i in range(len(self.W1))]  \n",
    "\n",
    "        # We don't have return statement as weights are updated in place!\n",
    "\n",
    "    # Compute the binary cross-entropy loss\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        m = len(y)\n",
    "        loss = 0\n",
    "        for i in range(m):\n",
    "            # Avoid log(0) by adding a small constant\n",
    "            loss += - (y[i] * math.log(y_pred[i][0] + 1e-15) + (1 - y[i]) * math.log(1 - y_pred[i][0] + 1e-15))\n",
    "        return loss / m\n",
    "\n",
    "\n",
    "    # Train the neural network\n",
    "    def train_batch(self, X, y, epochs=1000):\n",
    "        \"\"\" Batch Gradient Descent Training: Update all weights after processing the entire dataset.\"\"\"\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, y)     # weights are updated at this step\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "    # Predict class labels\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for given input data X.\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Convert DataFrame to list of lists for processing\n",
    "    X_list = X.values.tolist()\n",
    "    y_list = y.values.tolist()\n",
    "\n",
    "    # Initialize neural network\n",
    "    input_size = len(X.columns)  # Number of features\n",
    "    hidden_size = 5               # Number of hidden neurons (hyperparameter)\n",
    "    output_size = 1               # Binary classification\n",
    "    learning_rate = 0.1           # Learning rate (hyperparameter)\n",
    "\n",
    "    nn = SimpleNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "    # Train with different GD variants\n",
    "    print(\"Training with Batch GD:\")\n",
    "\n",
    "    # Train the neural network using batch gradient descent\n",
    "    epochs = 1000\n",
    "    loss_history = nn.train_batch(X_list, y_list, epochs)\n",
    "\n",
    "    # Plot the loss over epochs\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_pred = nn.predict(X_list)\n",
    "    y_pred_classes = [1 if pred[0] >= 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(1 for true, pred in zip(y_list, y_pred_classes) if true == pred) / len(y_list)\n",
    "    print(f'Training Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # calculate auc-roc score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_roc = roc_auc_score(y_list, [pred[0] for pred in y_pred])\n",
    "    print(f'AUC-ROC Score: {auc_roc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c19496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_before shape (samples x outputs): 2 1\n",
      "y_pred_after shape (samples x outputs): 2 1\n",
      "Loss before: 0.719376, Loss after: 0.716724\n"
     ]
    }
   ],
   "source": [
    "# Small NumPy-based test and demo (vectorized) - lightweight unit check\n",
    "import numpy as np\n",
    "\n",
    "# Create a tiny synthetic dataset\n",
    "X_test = np.array([[0.1, 0.2, 0.3],\n",
    "                   [0.4, 0.5, 0.6]])\n",
    "y_test = np.array([0, 1])\n",
    "\n",
    "# Simple shapes check for the list-based SimpleNN\n",
    "X_list = X_test.tolist()\n",
    "y_list = y_test.tolist()\n",
    "\n",
    "# Initialize a small network with input_size=3 features\n",
    "nn_small = SimpleNN(input_size=3, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "\n",
    "# Run forward and backward one step to ensure no index errors and reasonable shapes\n",
    "y_pred_before = nn_small.forward(X_list)\n",
    "print('y_pred_before shape (samples x outputs):', len(y_pred_before), len(y_pred_before[0]))\n",
    "\n",
    "# Perform a single backward/update step\n",
    "nn_small.backward(X_list, y_list)\n",
    "\n",
    "# Forward again\n",
    "y_pred_after = nn_small.forward(X_list)\n",
    "print('y_pred_after shape (samples x outputs):', len(y_pred_after), len(y_pred_after[0]))\n",
    "\n",
    "# Quick sanity: check loss decreased (may not strictly decrease after one step, but it shouldn't crash)\n",
    "loss_before = nn_small.compute_loss(y_list, y_pred_before)\n",
    "loss_after = nn_small.compute_loss(y_list, y_pred_after)\n",
    "print(f'Loss before: {loss_before:.6f}, Loss after: {loss_after:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac09bd",
   "metadata": {},
   "source": [
    "## Logistic Regression from scratch (vectorized, NumPy)\n",
    "\n",
    "This section implements a logistic regression binary classifier trained with gradient descent using NumPy vectorized operations (no scikit-learn or statsmodels). The implementation includes:\n",
    "\n",
    "- a small, well-documented `LogisticRegressionGD` class\n",
    "- numerically stable sigmoid and log-loss calculations\n",
    "- a simple demo that trains on a synthetic, linearly separable dataset and reports loss and accuracy\n",
    "\n",
    "Use the demo cell to run the training and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656db741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    \"\"\"Logistic Regression using batch Gradient Descent (vectorized, NumPy).\n",
    "\n",
    "    Simple, readable implementation suitable for teaching and small datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate (step size).\n",
    "    epochs : int\n",
    "        Number of gradient descent steps.\n",
    "    fit_intercept : bool\n",
    "        If True, add an intercept term.\n",
    "    verbose : int\n",
    "        Print progress every `verbose` epochs if > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.1, epochs=1000, fit_intercept=True, verbose=0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.coef_ = None  # weights (including intercept if fit_intercept=True)\n",
    "        self.loss_history_ = []\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        if not self.fit_intercept:\n",
    "            return X\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        # numerically stable sigmoid\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_loss(y, p):\n",
    "        # binary cross-entropy (averaged)\n",
    "        eps = 1e-15\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"Fit model using batch gradient descent.\n",
    "\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,) with binary labels {0,1}\n",
    "        X_val, y_val : optional validation set to track val loss\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1, 1)\n",
    "\n",
    "        X = self._add_intercept(X)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # initialize weights to zeros\n",
    "        self.coef_ = np.zeros((n_features, 1), dtype=float)\n",
    "\n",
    "        self.loss_history_ = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            # linear combination\n",
    "            z = X.dot(self.coef_)\n",
    "            # predictions\n",
    "            p = self._sigmoid(z)\n",
    "\n",
    "            # gradient: X^T (p - y) / n\n",
    "            grad = X.T.dot(p - y) / n_samples\n",
    "\n",
    "            # update\n",
    "            self.coef_ -= self.lr * grad\n",
    "\n",
    "            # loss tracking\n",
    "            loss = self._log_loss(y, p)\n",
    "            self.loss_history_.append(loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                p_val = self._sigmoid(self._add_intercept(np.asarray(X_val)).dot(self.coef_))\n",
    "                val_loss_history.append(self._log_loss(np.asarray(y_val).reshape(-1,1), p_val))\n",
    "\n",
    "            if self.verbose and (epoch % self.verbose == 0 or epoch == 1 or epoch == self.epochs):\n",
    "                msg = f\"Epoch {epoch}/{self.epochs} - loss: {loss:.6f}\"\n",
    "                if val_loss_history:\n",
    "                    msg += f\" - val_loss: {val_loss_history[-1]:.6f}\"\n",
    "                print(msg)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        X = self._add_intercept(X)\n",
    "        z = X.dot(self.coef_)\n",
    "        p = self._sigmoid(z)\n",
    "        return p.ravel()\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        p = self.predict_proba(X)\n",
    "        return (p >= threshold).astype(int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329dbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression on the preprocessed DataFrame `X` and target `y`\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert DataFrame/Series to lists (for legacy code) and to NumPy arrays (for training)\n",
    "X_list = X.values.tolist()\n",
    "y_list = y.values.tolist()\n",
    "\n",
    "X_np = np.asarray(X_list, dtype=float)\n",
    "y_np = np.asarray(y_list, dtype=float).ravel()\n",
    "\n",
    "# Quick train/validation split (stratified if labels are imbalanced)\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_np, y_np, test_size=0.2, random_state=42, stratify=y_np)\n",
    "except Exception:\n",
    "    # If sklearn not available or stratify fails (e.g., single class), fall back to simple split\n",
    "    n = X_np.shape[0]\n",
    "    split = int(0.8 * n)\n",
    "    X_train, X_val = X_np[:split], X_np[split:]\n",
    "    y_train, y_val = y_np[:split], y_np[split:]\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}\")\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegressionGD(lr=0.1, epochs=1000, fit_intercept=True, verbose=200)\n",
    "model.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = model.score(X_train, y_train)\n",
    "val_acc = model.score(X_val, y_val)\n",
    "print(f\"Train accuracy: {train_acc*100:.2f}% | Val accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "# Try ROC-AUC if sklearn available\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    y_val_proba = model.predict_proba(X_val)\n",
    "    auc = roc_auc_score(y_val, y_val_proba)\n",
    "    print(f\"Validation ROC-AUC: {auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print('ROC-AUC not computed:', e)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(model.loss_history_, label='train loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Note: decision-boundary plot omitted because this dataset may have >2 features\n",
    "# If you want a 2D projection, perform PCA or select two features and re-run predict_proba on a grid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
