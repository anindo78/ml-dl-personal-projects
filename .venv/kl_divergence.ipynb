{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b5fa3c",
   "metadata": {},
   "source": [
    
    "\n",
    "What is KL divergence (Kullback–Leibler)?\n",
    "=========================================\n",
    "\n",
    "KL divergence measures how one probability distribution PPP differs from another QQQ that’s treated as a reference. Intuition: “How many extra nats/bits does it cost to encode data from PPP if I use a code optimized for QQQ instead of PPP?”\n",
    "\n",
    "*   **Zero iff identical:** DKL(P∥Q)=0D\\_{\\\\mathrm{KL}}(P\\\\|Q)=0DKL​(P∥Q)=0 only when P=QP=QP=Q (almost everywhere).\n",
    "    \n",
    "*   **Asymmetric:** DKL(P∥Q)≠DKL(Q∥P)D\\_{\\\\mathrm{KL}}(P\\\\|Q) \\\\neq D\\_{\\\\mathrm{KL}}(Q\\\\|P)DKL​(P∥Q)=DKL​(Q∥P).\n",
    "    \n",
    "*   **Non-negative:** DKL≥0D\\_{\\\\mathrm{KL}}\\\\ge 0DKL​≥0.\n",
    "    \n",
    "*   **Units:** Natural log ⇒ **nats**; log base 2 ⇒ **bits**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62e086",
   "metadata": {},
   "source": [
    "What is KL divergence (Kullback–Leibler)?                                                                 \n",
    "  =========================================                                                                 \n",
    "                                                                                                            \n",
    "  KL divergence measures how one probability distribution $P$ differs from another $Q$ (taken as the        \n",
    "  reference). Intuition: “How many extra nats/bits does it cost to encode data from $P$ if I use a code     \n",
    "  optimized for $Q$ instead of $P$?”\n",
    "\n",
    "  - **Zero iff identical:** $D_{\\mathrm{KL}}(P \\parallel Q) = 0$ only when $P = Q$ (almost everywhere).     \n",
    "  - **Asymmetric:** $D_{\\mathrm{KL}}(P \\parallel Q) \\neq D_{\\mathrm{KL}}(Q \\parallel P)$. \n",
    "  [Explained here](https://www.youtube.com/watch?v=C_dKimu42D8)\n",
    "  - **Non-negative:** $D_{\\mathrm{KL}} \\ge 0$.\n",
    "  - **Units:** Natural log ⇒ **nats**; log base 2 ⇒ **bits**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21631a4",
   "metadata": {},
   "source": [
    "                                                                                                          \n",
    "  Why/where it’s used                                                                                       \n",
    "  -------------------                                                                                       \n",
    "                                                                                                            \n",
    "  - **Model fitting / cross-entropy loss:** Minimizing                                                      \n",
    "    $$                                                                                                      \n",
    "    \\mathbb{E}_{x \\sim P}\\left[-\\log Q(x)\\right]                                                            \n",
    "    $$\n",
    "    is equivalent to minimizing $D_{\\mathrm{KL}}(P \\parallel Q)$ since                                      \n",
    "    $$                                                                                                      \n",
    "    CE(P,Q) = H(P) + D_{\\mathrm{KL}}(P \\parallel Q).                                                        \n",
    "    $$                                                                                                      \n",
    "                                                                                                            \n",
    "  - **Variational Inference (ELBO):** Fit $q_{\\phi}(z)$ to posterior $p(z \\mid x)$ by minimizing            \n",
    "    $D_{\\mathrm{KL}}\\!\\left(q_{\\phi}(z) \\parallel p(z \\mid x)\\right)$.                                      \n",
    "                                                                                                            \n",
    "  - **Distribution shift & calibration:** Compare predicted class distributions or priors between           \n",
    "  environments.                                                                                             \n",
    "                                                                                                            \n",
    "  - **RL / Policy updates:** Trust-region methods (TRPO, PPO) constrain the KL divergence between old and   \n",
    "  new policies.                                                                                             \n",
    "                                                                                                            \n",
    "  - **Information theory:** Measures the expected code-length regret incurred when using the wrong          \n",
    "  distribution.                                                                                             \n",
    "                                                                                                            \n",
    "  The math                                                                                                  \n",
    "  --------                                                                                                  \n",
    "                                                                                                            \n",
    "  ### Discrete case                                                                                         \n",
    "                                                                                                            \n",
    "  $$                                                                                                        \n",
    "  D_{\\mathrm{KL}}(P \\parallel Q) = \\sum_{i} p_i \\log \\frac{p_i}{q_i}                                        \n",
    "  $$                                                                                                        \n",
    "                                                                                                            \n",
    "  Important: if $q_i = 0$ while $p_i > 0$, KL divergence is $+\\infty$.                                      \n",
    "                                                                                                            \n",
    "  **Cross-entropy relation**                                                                                \n",
    "                                                                                                            \n",
    "  $$                                                                                                        \n",
    "  CE(P, Q) \\equiv -\\sum_{i} p_i \\log q_i = H(P) + D_{\\mathrm{KL}}(P \\parallel Q).                           \n",
    "  $$                                                                                                        \n",
    "                                                                                                            \n",
    "  ### Continuous case                                                                                       \n",
    "                                                                                                            \n",
    "  $$                                                                                                        \n",
    "  D_{\\mathrm{KL}}(P \\parallel Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx                                   \n",
    "  $$                                                                                                        \n",
    "                                                                                                            \n",
    "  ### Closed form for univariate Gaussians                                                                  \n",
    "                                                                                                            \n",
    "  Let $P = \\mathcal{N}(\\mu_0, \\sigma_0^2)$ and $Q = \\mathcal{N}(\\mu_1, \\sigma_1^2)$:                        \n",
    "                                                                                                            \n",
    "  $$                                                                                                        \n",
    "  D_{\\mathrm{KL}}(P \\parallel Q) =                                                                          \n",
    "  \\log \\frac{\\sigma_1}{\\sigma_0}                                                                            \n",
    "  + \\frac{\\sigma_0^{2} + (\\mu_0 - \\mu_1)^2}{2\\sigma_1^{2}}                                                  \n",
    "  - \\frac{1}{2}.                                                                                            \n",
    "  $$                                                                                                        \n",
    "                                                                                                            \n",
    "  ### Jensen–Shannon divergence                                                                             \n",
    "                                                                                                            \n",
    "  A symmetric, bounded alternative often used for evaluation:                                               \n",
    "                                                                                                            \n",
    "  $$                                                                                                        \n",
    "  JS(P, Q) = \\frac{1}{2} D_{\\mathrm{KL}}(P \\parallel M) + \\frac{1}{2} D_{\\mathrm{KL}}(Q \\parallel M),       \n",
    "  \\quad                                                                                                     \n",
    "  M = \\frac{P + Q}{2}.                                                                                      \n",
    "  $$                                                                                                        \n",
    "\n",
    "  $JS$ is symmetric and lies in $[0, \\log 2]$ nats (or $[0, 1]$ bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e143aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence: reusable functions + worked examples\n",
    "import numpy as np\n",
    "\n",
    "def _as_prob_vec(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Normalize to a probability vector and clip to avoid log(0).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"x must be a 1D array of nonnegative counts or probabilities.\")\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"x has negative entries.\")\n",
    "    s = x.sum()\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"Sum must be positive.\")\n",
    "    p = x / s\n",
    "    # clip to avoid zeros; preserves normalization approximately\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    p = p / p.sum()\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa036787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_discrete(p, q, base=None, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    D_KL(P || Q) = sum_i p_i * log(p_i / q_i)\n",
    "    If base is None -> natural log (nats). For bits, set base=2.\n",
    "    p, q can be probability vectors OR nonnegative counts.\n",
    "    \"\"\"\n",
    "    p = _as_prob_vec(np.asarray(p), eps=eps)\n",
    "    q = _as_prob_vec(np.asarray(q), eps=eps)\n",
    "    if p.shape != q.shape:\n",
    "        raise ValueError(\"p and q must have the same shape.\")\n",
    "    log_ratio = np.log(p) - np.log(q)\n",
    "    kl = float(np.sum(p * log_ratio))\n",
    "    if base is not None:\n",
    "        kl = kl / np.log(base)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jensen_shannon(p, q, base=None, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    JS(P, Q) = 0.5*KL(P||M) + 0.5*KL(Q||M), M=(P+Q)/2.\n",
    "    Symmetric and bounded in [0, log(2)] for natural logs.\n",
    "    \"\"\"\n",
    "    p = _as_prob_vec(np.asarray(p), eps=eps)\n",
    "    q = _as_prob_vec(np.asarray(q), eps=eps)\n",
    "    m = 0.5 * (p + q)\n",
    "    js = 0.5 * kl_divergence_discrete(p, m, base=base, eps=eps) + \\\n",
    "         0.5 * kl_divergence_discrete(q, m, base=base, eps=eps)\n",
    "    return float(js)\n",
    "\n",
    "def entropy(p, base=None, eps: float = 1e-12):\n",
    "    p = _as_prob_vec(np.asarray(p), eps=eps)\n",
    "    H = -float(np.sum(p * np.log(p)))\n",
    "    if base is not None:\n",
    "        H = H / np.log(base)\n",
    "    return H\n",
    "\n",
    "def cross_entropy(p, q, base=None, eps: float = 1e-12):\n",
    "    p = _as_prob_vec(np.asarray(p), eps=eps)\n",
    "    q = _as_prob_vec(np.asarray(q), eps=eps)\n",
    "    Hpq = -float(np.sum(p * np.log(q)))\n",
    "    if base is not None:\n",
    "        Hpq = Hpq / np.log(base)\n",
    "    return Hpq\n",
    "\n",
    "\n",
    "# Analytic KL for univariate Gaussians: P=N(mu0, s0^2), Q=N(mu1, s1^2)\n",
    "def kl_normal_1d(mu0, s0, mu1, s1, base=None):\n",
    "    if s0 <= 0 or s1 <= 0:\n",
    "        raise ValueError(\"Standard deviations must be positive.\")\n",
    "    term = np.log(s1 / s0) + (s0**2 + (mu0 - mu1)**2) / (2 * s1**2) - 0.5\n",
    "    if base is not None:\n",
    "        term = term / np.log(base)\n",
    "    return float(term)\n",
    "\n",
    "# Monte Carlo estimate of KL(P||Q) from samples x~P and log-densities\n",
    "def kl_mc_from_samples(logp, logq, x):\n",
    "    \"\"\"\n",
    "    logp, logq: callables returning log-density at x.\n",
    "    x: samples ~ P\n",
    "    \"\"\"\n",
    "    lp = logp(x)\n",
    "    lq = logq(x)\n",
    "    return float(np.mean(lp - lq))\n",
    "\n",
    "def logpdf_normal_1d(x, mu, s):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return -0.5*np.log(2*np.pi) - np.log(s) - 0.5*((x - mu)/s)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e05957",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 1) Bernoulli example (binary classification target shift)\n",
    "p = np.array([0.7, 0.3])  # \"true\" positive/negative mix\n",
    "q = np.array([0.5, 0.5])  # model prior or production mix\n",
    "results['bernoulli_kl_nats'] = kl_divergence_discrete(p, q)\n",
    "results['bernoulli_kl_bits'] = kl_divergence_discrete(p, q, base=2)\n",
    "results['bernoulli_entropy_bits'] = entropy(p, base=2)\n",
    "results['bernoulli_cross_entropy_bits'] = cross_entropy(p, q, base=2)  # should equal H(p)+KL in bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc27b4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli_kl_nats           : 0.082283\n",
      "bernoulli_kl_bits           : 0.118709\n",
      "bernoulli_entropy_bits      : 0.881291\n",
      "bernoulli_cross_entropy_bits: 1.000000\n",
      "categorical_KL_P||Q         : 0.091516\n",
      "categorical_KL_Q||P         : 0.104650\n",
      "categorical_JS              : 0.024157\n",
      "gaussian_analytic           : 0.349910\n",
      "gaussian_mc_estimate        : 0.350000\n",
      "gaussian_abs_error_mc       : 0.000090\n",
      "support_mismatch_demo       : 24.542836\n",
      "\n",
      "Interpretation notes:\n",
      "* KL is in nats unless 'base=2' was used (then in bits).\n",
      "* Cross-entropy H(p,q) = H(p) + KL(p||q) (see Bernoulli bits above).\n",
      "* KL is asymmetric: KL(P||Q) != KL(Q||P) (see categorical example).\n",
      "* JS(P,Q) is symmetric and bounded in [0, ln 2] nats (or [0,1] bits).\n",
      "* Monte Carlo estimate closely matches analytic KL for Gaussians.\n",
      "* If Q assigns zero probability where P has mass, true KL is +infinity; clipping produces a large finite surrogate.\n"
     ]
    }
   ],
   "source": [
    "# 2) Multiclass distributions (e.g., two models' predictions over 3 classes)\n",
    "p3 = np.array([0.80, 0.15, 0.05])  # Model A\n",
    "q3 = np.array([0.60, 0.30, 0.10])  # Model B\n",
    "results['categorical_KL_P||Q'] = kl_divergence_discrete(p3, q3)\n",
    "results['categorical_KL_Q||P'] = kl_divergence_discrete(q3, p3)  # asymmetry\n",
    "results['categorical_JS'] = jensen_shannon(p3, q3)\n",
    "\n",
    "# 3) Univariate Gaussians: analytic vs Monte Carlo\n",
    "mu0, s0 = 0.0, 1.0\n",
    "mu1, s1 = 1.0, 1.5\n",
    "kl_analytic = kl_normal_1d(mu0, s0, mu1, s1)\n",
    "# Monte Carlo\n",
    "rng = np.random.default_rng(7)\n",
    "x = rng.normal(mu0, s0, size=200000)\n",
    "logp = lambda z: logpdf_normal_1d(z, mu0, s0)\n",
    "logq = lambda z: logpdf_normal_1d(z, mu1, s1)\n",
    "kl_mc = kl_mc_from_samples(logp, logq, x)\n",
    "results['gaussian_analytic'] = kl_analytic\n",
    "results['gaussian_mc_estimate'] = kl_mc\n",
    "results['gaussian_abs_error_mc'] = abs(kl_mc - kl_analytic)\n",
    "\n",
    "# 4) Practical caution: support mismatch (zero-probability under Q)\n",
    "p_supporty = np.array([0.9, 0.1, 0.0])\n",
    "q_zeros =     np.array([0.0, 1.0, 0.0])  # assigns zero prob where P has mass -> KL -> +inf (before clipping)\n",
    "kl_with_clip = kl_divergence_discrete(p_supporty, q_zeros)  # with epsilon clipping\n",
    "results['support_mismatch_demo'] = kl_with_clip\n",
    "\n",
    "# Pretty-print\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:28s}: {v:.6f}\")\n",
    "\n",
    "print(\"\\nInterpretation notes:\")\n",
    "print(\"* KL is in nats unless 'base=2' was used (then in bits).\")\n",
    "print(\"* Cross-entropy H(p,q) = H(p) + KL(p||q) (see Bernoulli bits above).\")\n",
    "print(\"* KL is asymmetric: KL(P||Q) != KL(Q||P) (see categorical example).\")\n",
    "print(\"* JS(P,Q) is symmetric and bounded in [0, ln 2] nats (or [0,1] bits).\")\n",
    "print(\"* Monte Carlo estimate closely matches analytic KL for Gaussians.\")\n",
    "print(\"* If Q assigns zero probability where P has mass, true KL is +infinity; clipping produces a large finite surrogate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c989eb",
   "metadata": {},
   "source": [
    "**KL Divergence Interpretation**\n",
    "\n",
    "- Bernoulli KL ≈ $_0.082\\text{ nats }(0.119\\text{ bits})$_ → small difference between true and model distributions.  \n",
    "- Entropy $_H(P)=0.881\\text{ bits}$_ → inherent uncertainty of the true distribution.  \n",
    "- Cross-entropy $_H(P,Q)=1.00\\text{ bits}$_ → encoding cost using model $Q$.  \n",
    "- Relation: $_H(P,Q)=H(P)+D_{KL}(P\\|Q)$.  \n",
    "- Asymmetry: $_D_{KL}(P\\|Q)\\neq D_{KL}(Q\\|P)$_ (0.092 vs 0.105).  \n",
    "- JS Divergence $_=0.024$_ → symmetric, small → distributions are similar.  \n",
    "- Gaussian KL $_\\approx0.35$_ nats → moderate shift in mean/variance; Monte-Carlo check matches analytic.  \n",
    "- Support mismatch $_\\approx24.5$_ → practically infinite; $Q$ misses mass where $P$ has probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10d959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
