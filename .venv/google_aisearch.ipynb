{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f062f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anind\\Documents\\python-projects\\ml-dl-personal-projects\\.venv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week_period</th>\n",
       "      <th>category</th>\n",
       "      <th>query</th>\n",
       "      <th>long_tail</th>\n",
       "      <th>ai_overview</th>\n",
       "      <th>actions_taken</th>\n",
       "      <th>ads_viewed</th>\n",
       "      <th>ads_clicked</th>\n",
       "      <th>cpc_usd</th>\n",
       "      <th>google_revenue_usd</th>\n",
       "      <th>time_spent_search_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>upcoming releases thriller 2025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>refine_query</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Travel</td>\n",
       "      <td>public transport pass for Paris</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>visit_publisher</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Tech</td>\n",
       "      <td>React tutorial step by step</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>visit_ads</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date week_period       category                            query  \\\n",
       "0  2025-04-28         pre  Entertainment  upcoming releases thriller 2025   \n",
       "1  2025-04-28         pre         Travel  public transport pass for Paris   \n",
       "2  2025-04-28         pre           Tech      React tutorial step by step   \n",
       "\n",
       "   long_tail  ai_overview    actions_taken  ads_viewed  ads_clicked  cpc_usd  \\\n",
       "0      False        False     refine_query           1            0     0.52   \n",
       "1      False        False  visit_publisher           0            0     0.65   \n",
       "2       True        False        visit_ads           3            0     0.65   \n",
       "\n",
       "   google_revenue_usd  time_spent_search_sec  \n",
       "0                 0.0                   14.2  \n",
       "1                 0.0                   12.0  \n",
       "2                 0.0                   35.8  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# read the csv\n",
    "df = pd.read_csv(\"synthetic_search_ai_overviews_24w.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f8cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   date                   120000 non-null  object \n",
      " 1   week_period            120000 non-null  object \n",
      " 2   category               120000 non-null  object \n",
      " 3   query                  120000 non-null  object \n",
      " 4   long_tail              120000 non-null  bool   \n",
      " 5   ai_overview            120000 non-null  bool   \n",
      " 6   actions_taken          120000 non-null  object \n",
      " 7   ads_viewed             120000 non-null  int64  \n",
      " 8   ads_clicked            120000 non-null  int64  \n",
      " 9   cpc_usd                120000 non-null  float64\n",
      " 10  google_revenue_usd     120000 non-null  float64\n",
      " 11  time_spent_search_sec  120000 non-null  float64\n",
      "dtypes: bool(2), float64(3), int64(2), object(5)\n",
      "memory usage: 9.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# exploratory data analysis\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5f2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week_period</th>\n",
       "      <th>category</th>\n",
       "      <th>query</th>\n",
       "      <th>long_tail</th>\n",
       "      <th>ai_overview</th>\n",
       "      <th>actions_taken</th>\n",
       "      <th>ads_viewed</th>\n",
       "      <th>ads_clicked</th>\n",
       "      <th>cpc_usd</th>\n",
       "      <th>google_revenue_usd</th>\n",
       "      <th>time_spent_search_sec</th>\n",
       "      <th>post</th>\n",
       "      <th>treated_ai</th>\n",
       "      <th>week</th>\n",
       "      <th>query_len_words</th>\n",
       "      <th>query_len_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>upcoming releases thriller 2025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>refine_query</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-28/2025-05-04</td>\n",
       "      <td>4</td>\n",
       "      <td>medium(4–6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Travel</td>\n",
       "      <td>public transport pass for Paris</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>visit_publisher</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-28/2025-05-04</td>\n",
       "      <td>5</td>\n",
       "      <td>medium(4–6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>pre</td>\n",
       "      <td>Tech</td>\n",
       "      <td>React tutorial step by step</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>visit_ads</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-04-28/2025-05-04</td>\n",
       "      <td>5</td>\n",
       "      <td>medium(4–6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date week_period       category                            query  \\\n",
       "0 2025-04-28         pre  Entertainment  upcoming releases thriller 2025   \n",
       "1 2025-04-28         pre         Travel  public transport pass for Paris   \n",
       "2 2025-04-28         pre           Tech      React tutorial step by step   \n",
       "\n",
       "   long_tail  ai_overview    actions_taken  ads_viewed  ads_clicked  cpc_usd  \\\n",
       "0      False        False     refine_query           1            0     0.52   \n",
       "1      False        False  visit_publisher           0            0     0.65   \n",
       "2       True        False        visit_ads           3            0     0.65   \n",
       "\n",
       "   google_revenue_usd  time_spent_search_sec  post  treated_ai  \\\n",
       "0                 0.0                   14.2     0           0   \n",
       "1                 0.0                   12.0     0           0   \n",
       "2                 0.0                   35.8     0           0   \n",
       "\n",
       "                    week  query_len_words query_len_bucket  \n",
       "0  2025-04-28/2025-05-04                4      medium(4–6)  \n",
       "1  2025-04-28/2025-05-04                5      medium(4–6)  \n",
       "2  2025-04-28/2025-05-04                5      medium(4–6)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# convert data into datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# Recreate helper fields if needed\n",
    "for col, func in [\n",
    "    (\"post\", lambda d: (d[\"week_period\"] == \"post\").astype(int)),\n",
    "    (\"treated_ai\", lambda d: d[\"ai_overview\"].astype(int)),\n",
    "    (\"week\", lambda d: d[\"date\"].dt.to_period(\"W\").astype(str)),\n",
    "    (\"query_len_words\", lambda d: d[\"query\"].str.split().str.len()),\n",
    "]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = func(df)\n",
    "\n",
    "\n",
    "if \"query_len_bucket\" not in df.columns:\n",
    "    def length_bucket(n):\n",
    "        if n <= 3: return \"short(≤3)\"\n",
    "        elif n <= 6: return \"medium(4–6)\"\n",
    "        else: return \"long(≥7)\"\n",
    "    df[\"query_len_bucket\"] = df[\"query_len_words\"].apply(length_bucket)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e01c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120000.000000\n",
       "mean         23.624751\n",
       "std          14.989685\n",
       "min           1.800000\n",
       "25%          13.500000\n",
       "50%          19.900000\n",
       "75%          29.500000\n",
       "max         235.900000\n",
       "Name: time_spent_search_sec, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"time_spent_search_sec\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73198444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for modeling the search time using Weibull, Gamme and Log-Normal distributions\n",
    "# 1) Fits Weibull, Lognormal, and Gamma to time_spent_search_sec\n",
    "# 2) Reports KS goodness-of-fit, AIC, and BIC for model comparison\n",
    "# 3) Runs the same tests group-wise for ai_overview (both overall and post-period only)\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Keep strictly positive times (KS/AIC/BIC assume continuous positive support)\n",
    "x_all = df[\"time_spent_search_sec\"].astype(float)\n",
    "x_all = x_all[x_all > 0].values\n",
    "\n",
    "\n",
    "# ========= Core helpers =========\n",
    "def fit_and_scores(x, dist_name):\n",
    "    \"\"\"\n",
    "    Fit a distribution to sample x and compute KS statistic+p-value, log-likelihood, AIC, BIC.\n",
    "    Uses SciPy continuous dists:\n",
    "      - 'weibull_min'  (Weibull, with shape c)\n",
    "      - 'lognorm'      (Lognormal, with shape=sigma)\n",
    "      - 'gamma'        (Gamma, with shape=k)\n",
    "    \"\"\"\n",
    "    if dist_name == \"weibull\":\n",
    "        dist = stats.weibull_min\n",
    "    elif dist_name == \"lognorm\":\n",
    "        dist = stats.lognorm\n",
    "    elif dist_name == \"gamma\":\n",
    "        dist = stats.gamma\n",
    "    else:\n",
    "        raise ValueError(\"dist_name must be 'weibull', 'lognorm', or 'gamma'\")\n",
    "\n",
    "    # Fit: allow location and scale to float\n",
    "    params = dist.fit(x)  # returns tuple (shape(s), loc, scale)\n",
    "\n",
    "    # KS test against fitted CDF\n",
    "    # kstest expects a callable CDF with the fitted params\n",
    "    D, p = stats.kstest(x, lambda v: dist.cdf(v, *params))\n",
    "\n",
    "    # Log-likelihood\n",
    "    ll = np.sum(dist.logpdf(x, *params))\n",
    "    n = len(x)\n",
    "    k = len(params)  # number of free params\n",
    "    aic = 2 * k - 2 * ll\n",
    "    bic = k * np.log(n) - 2 * ll\n",
    "\n",
    "    return {\n",
    "        \"distribution\": dist_name,\n",
    "        \"n\": n,\n",
    "        \"params\": params,\n",
    "        \"KS_stat\": D,\n",
    "        \"KS_pvalue\": p,\n",
    "        \"loglik\": ll,\n",
    "        \"AIC\": aic,\n",
    "        \"BIC\": bic\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f86c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(x, label=\"overall\"):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x) & (x > 0)]\n",
    "    results = []\n",
    "    for d in [\"weibull\", \"lognorm\", \"gamma\"]:\n",
    "        results.append(fit_and_scores(x, d))\n",
    "    out = pd.DataFrame(results).sort_values([\"AIC\", \"BIC\"]).reset_index(drop=True)\n",
    "    out.insert(0, \"sample\", label)\n",
    "    out.insert(1, \"rank\", np.arange(1, len(out) + 1))\n",
    "    alpha = 0.05\n",
    "    out[\"KS_fit_comment\"] = out[\"KS_pvalue\"].apply(\n",
    "        lambda p: f\"Fail to reject KS null (good fit at alpha={alpha})\" if p >= alpha else \"\"\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e785b97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>rank</th>\n",
       "      <th>distribution</th>\n",
       "      <th>n</th>\n",
       "      <th>params</th>\n",
       "      <th>KS_stat</th>\n",
       "      <th>KS_pvalue</th>\n",
       "      <th>loglik</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>KS_fit_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL (pre+post)</td>\n",
       "      <td>1</td>\n",
       "      <td>lognorm</td>\n",
       "      <td>120000</td>\n",
       "      <td>(0.5855909121046509, 0.12619490015501816, 19.7...</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>1.599013e-01</td>\n",
       "      <td>-464333.150949</td>\n",
       "      <td>928672.301898</td>\n",
       "      <td>928701.387639</td>\n",
       "      <td>Fail to reject KS null (good fit at alpha=0.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALL (pre+post)</td>\n",
       "      <td>2</td>\n",
       "      <td>gamma</td>\n",
       "      <td>120000</td>\n",
       "      <td>(2.5806180583836085, 1.797753856980107, 8.4580...</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>5.233693e-135</td>\n",
       "      <td>-466314.941644</td>\n",
       "      <td>932635.883288</td>\n",
       "      <td>932664.969029</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL (pre+post)</td>\n",
       "      <td>3</td>\n",
       "      <td>weibull</td>\n",
       "      <td>120000</td>\n",
       "      <td>(1.592691659647254, 1.7996844045875882, 24.502...</td>\n",
       "      <td>0.052818</td>\n",
       "      <td>2.156590e-291</td>\n",
       "      <td>-470891.726800</td>\n",
       "      <td>941789.453600</td>\n",
       "      <td>941818.539341</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample  rank distribution       n  \\\n",
       "0  ALL (pre+post)     1      lognorm  120000   \n",
       "1  ALL (pre+post)     2        gamma  120000   \n",
       "2  ALL (pre+post)     3      weibull  120000   \n",
       "\n",
       "                                              params   KS_stat      KS_pvalue  \\\n",
       "0  (0.5855909121046509, 0.12619490015501816, 19.7...  0.003243   1.599013e-01   \n",
       "1  (2.5806180583836085, 1.797753856980107, 8.4580...  0.035927  5.233693e-135   \n",
       "2  (1.592691659647254, 1.7996844045875882, 24.502...  0.052818  2.156590e-291   \n",
       "\n",
       "          loglik            AIC            BIC  \\\n",
       "0 -464333.150949  928672.301898  928701.387639   \n",
       "1 -466314.941644  932635.883288  932664.969029   \n",
       "2 -470891.726800  941789.453600  941818.539341   \n",
       "\n",
       "                                    KS_fit_comment  \n",
       "0  Fail to reject KS null (good fit at alpha=0.05)  \n",
       "1                                                   \n",
       "2                                                   "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= 1) Overall GOF: which dist fits best? =========\n",
    "overall_gof = compare_distributions(x_all, label=\"ALL (pre+post)\")\n",
    "overall_gof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fbe083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OVERALL (ALL ROWS) — Best-fitting distribution by AIC/BIC ===\n",
      "        sample distribution      n  KS_stat     KS_pvalue           AIC           BIC                                                        params\n",
      "ALL (pre+post)      lognorm 120000 0.003243  1.599013e-01 928672.301898 928701.387639 (0.5855909121046509, 0.12619490015501816, 19.799134542508654)\n",
      "ALL (pre+post)        gamma 120000 0.035927 5.233693e-135 932635.883288 932664.969029    (2.5806180583836085, 1.797753856980107, 8.458047601826436)\n",
      "ALL (pre+post)      weibull 120000 0.052818 2.156590e-291 941789.453600 941818.539341   (1.592691659647254, 1.7996844045875882, 24.502105273262146)\n",
      "\n",
      "=== GROUP-WISE (ALL ROWS) ===\n",
      "       sample distribution     n  KS_stat    KS_pvalue           AIC           BIC                                                      params\n",
      "AI (all rows)      lognorm 18149 0.006501 4.251894e-01 130547.912853 130571.331965 (0.5870051472090899, 0.4513529957125645, 15.03193972191415)\n",
      "AI (all rows)        gamma 18149 0.037121 3.648507e-22 131157.485286 131180.904398 (2.498954688212588, 1.8797678080094058, 6.5765648748528065)\n",
      "AI (all rows)      weibull 18149 0.053585 9.832101e-46 132463.224364 132486.643476 (1.5679459790030936, 1.8970425006605365, 18.40290921538321)\n",
      "           sample distribution      n  KS_stat     KS_pvalue           AIC           BIC                                                       params\n",
      "Non-AI (all rows)      lognorm 101851 0.003745  1.146045e-01 794153.046500 794181.640298 (0.5762086109102357, 0.1140052034010504, 20.716275173811876)\n",
      "Non-AI (all rows)        gamma 101851 0.035866 2.860643e-114 797582.901603 797611.495401     (2.678960981879518, 1.7970684536659112, 8.5010449142131)\n",
      "Non-AI (all rows)      weibull 101851 0.052806 2.818572e-247 805748.067064 805776.660862    (1.62097053888085, 1.7995874098896651, 25.60252186255168)\n",
      "\n",
      "Two-sample KS (AI vs Non-AI, ALL ROWS): statistic=0.2060, p=0\n",
      "\n",
      "=== GROUP-WISE (POST-PERIOD ONLY) ===\n",
      "        sample distribution     n  KS_stat    KS_pvalue           AIC           BIC                                                      params\n",
      "AI (post only)      lognorm 18149 0.006501 4.251894e-01 130547.912853 130571.331965 (0.5870051472090899, 0.4513529957125645, 15.03193972191415)\n",
      "AI (post only)        gamma 18149 0.037121 3.648507e-22 131157.485286 131180.904398 (2.498954688212588, 1.8797678080094058, 6.5765648748528065)\n",
      "AI (post only)      weibull 18149 0.053585 9.832101e-46 132463.224364 132486.643476 (1.5679459790030936, 1.8970425006605365, 18.40290921538321)\n",
      "            sample distribution     n  KS_stat     KS_pvalue           AIC           BIC                                                       params\n",
      "Non-AI (post only)      lognorm 41851 0.005696  1.317903e-01 324986.395165 325012.320778 (0.5771469246085841, 0.14496620445722608, 20.35462274944261)\n",
      "Non-AI (post only)        gamma 41851 0.037231  7.710031e-51 326398.324639 326424.250252  (2.6076776190298636, 1.9928550643799534, 8.514709931040867)\n",
      "Non-AI (post only)      weibull 41851 0.054482 2.058464e-108 329655.113408 329681.039021 (1.5986080759307204, 1.9990625322987938, 24.926779203639278)\n",
      "\n",
      "Two-sample KS (AI vs Non-AI, POST only): statistic=0.1967, p=0\n"
     ]
    }
   ],
   "source": [
    "# ========= 2) Group-wise GOF: AI vs non-AI =========\n",
    "# (a) Across the entire dataset (non-AI includes pre + post non-AI)\n",
    "x_ai_all = df.loc[df[\"ai_overview\"]==True, \"time_spent_search_sec\"].values\n",
    "x_non_ai_all = df.loc[df[\"ai_overview\"]==False, \"time_spent_search_sec\"].values\n",
    "gof_ai_all = compare_distributions(x_ai_all, label=\"AI (all rows)\")\n",
    "gof_non_ai_all = compare_distributions(x_non_ai_all, label=\"Non-AI (all rows)\")\n",
    "\n",
    "# (b) Post-period only (to avoid time confounding)\n",
    "post = df[df[\"post\"]==1]\n",
    "x_ai_post = post.loc[post[\"ai_overview\"]==True, \"time_spent_search_sec\"].values\n",
    "x_non_ai_post = post.loc[post[\"ai_overview\"]==False, \"time_spent_search_sec\"].values\n",
    "gof_ai_post = compare_distributions(x_ai_post, label=\"AI (post only)\")\n",
    "gof_non_ai_post = compare_distributions(x_non_ai_post, label=\"Non-AI (post only)\")\n",
    "\n",
    "# ========= 3) Optional: Direct distributional difference test between AI vs Non-AI =========\n",
    "# KS two-sample (does not assume any parametric family)\n",
    "ks_all = stats.ks_2samp(x_ai_all, x_non_ai_all, alternative=\"two-sided\", method=\"auto\")\n",
    "ks_post = stats.ks_2samp(x_ai_post, x_non_ai_post, alternative=\"two-sided\", method=\"auto\")\n",
    "\n",
    "# ========= Display / print summaries =========\n",
    "def pretty(df_):\n",
    "    cols = [\"sample\",\"distribution\",\"n\",\"KS_stat\",\"KS_pvalue\",\"AIC\",\"BIC\",\"params\"]\n",
    "    return df_[cols].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== OVERALL (ALL ROWS) — Best-fitting distribution by AIC/BIC ===\")\n",
    "print(pretty(overall_gof).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== GROUP-WISE (ALL ROWS) ===\")\n",
    "print(pretty(gof_ai_all).to_string(index=False))\n",
    "print(pretty(gof_non_ai_all).to_string(index=False))\n",
    "print(f\"\\nTwo-sample KS (AI vs Non-AI, ALL ROWS): statistic={ks_all.statistic:.4f}, p={ks_all.pvalue:.4g}\")\n",
    "\n",
    "print(\"\\n=== GROUP-WISE (POST-PERIOD ONLY) ===\")\n",
    "print(pretty(gof_ai_post).to_string(index=False))\n",
    "print(pretty(gof_non_ai_post).to_string(index=False))\n",
    "print(f\"\\nTwo-sample KS (AI vs Non-AI, POST only): statistic={ks_post.statistic:.4f}, p={ks_post.pvalue:.4g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d31c201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quick winners by AIC ===\n",
      " - ALL (pre+post): lognorm (AIC=928672.3, BIC=928701.4)\n",
      " - AI (all rows): lognorm (AIC=130547.9, BIC=130571.3)\n",
      " - Non-AI (all rows): lognorm (AIC=794153.0, BIC=794181.6)\n",
      " - AI (post only): lognorm (AIC=130547.9, BIC=130571.3)\n",
      " - Non-AI (post only): lognorm (AIC=324986.4, BIC=325012.3)\n"
     ]
    }
   ],
   "source": [
    "# ========= (Optional) Quick pick of “winner” per table =========\n",
    "def winner(df_):\n",
    "    # Small helper to pick lowest AIC (ties broken by BIC)\n",
    "    w = df_.sort_values([\"AIC\",\"BIC\"]).iloc[0]\n",
    "    return f\"{w['sample']}: {w['distribution']} (AIC={w['AIC']:.1f}, BIC={w['BIC']:.1f})\"\n",
    "\n",
    "print(\"\\n=== Quick winners by AIC ===\")\n",
    "for tbl in [overall_gof, gof_ai_all, gof_non_ai_all, gof_ai_post, gof_non_ai_post]:\n",
    "    print(\" -\", winner(tbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e50dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treated_ai      0      1\n",
      "post                    \n",
      "0           60000      0\n",
      "1           41851  18149\n"
     ]
    }
   ],
   "source": [
    "cross_tab = pd.crosstab(df['post'], df['treated_ai'])\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c816dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e7c8041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>alpha</th>\n",
       "      <th>coef_post</th>\n",
       "      <th>pvalue_post</th>\n",
       "      <th>post_significant</th>\n",
       "      <th>coef_treated_ai (ATT)</th>\n",
       "      <th>pvalue_treated_ai (ATT)</th>\n",
       "      <th>treated_ai_significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>time_spent_search_sec</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.77835</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>yes</td>\n",
       "      <td>-4.801788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google_revenue_usd</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00417</td>\n",
       "      <td>0.4763</td>\n",
       "      <td>no</td>\n",
       "      <td>-0.040565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       y  alpha  coef_post  pvalue_post post_significant  \\\n",
       "0  time_spent_search_sec   0.05   -0.77835       0.0001              yes   \n",
       "1     google_revenue_usd   0.05   -0.00417       0.4763               no   \n",
       "\n",
       "   coef_treated_ai (ATT)  pvalue_treated_ai (ATT) treated_ai_significant  \n",
       "0              -4.801788                      0.0                    yes  \n",
       "1              -0.040565                      0.0                    yes  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "# ---------- (1) DiD via regression with category and week fixed effects ----------\n",
    "# Model: y = b0 + b1*post + b2*treated_ai + FE(category) + FE(week) + e\n",
    "# Note: treated_ai is equivalent to post*ai since ai=0 in pre; b2 ~ ATT of AI exposure in post.\n",
    "def did_regression(ycol, alpha=0.05):\n",
    "    # Build design with one-hot FE for category & week\n",
    "    X = df[[\"post\",\"treated_ai\",\"category\",\"week\"]].copy()\n",
    "    y = df[ycol].astype(float).values\n",
    "    ct = ColumnTransformer([\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), [\"category\",\"week\"])\n",
    "    ], remainder=\"passthrough\")\n",
    "\n",
    "    # Linear regression via OLS closed form using numpy (to avoid adding \n",
    "    #  sklearn linear reg which doesn't show coeff names easily)\n",
    "    X_enc = ct.fit_transform(X)\n",
    "\n",
    "    # X_enc is sparse; convert to dense for OLS (dataset size allows it)\n",
    "    X_mat = X_enc.toarray()\n",
    "\n",
    "    # Add intercept\n",
    "    X_design = np.column_stack([np.ones(X_mat.shape[0]), X_mat])\n",
    "\n",
    "    # OLS: beta = (X'X)^(-1) X'y\n",
    "    beta, _, rank, _ = np.linalg.lstsq(X_design, y, rcond=None)\n",
    "\n",
    "    # Residual variance and (X'X)^{-1} for standard errors\n",
    "    resid = y - X_design @ beta\n",
    "    df_resid = max(X_design.shape[0] - rank, 1)\n",
    "    sigma2 = (resid @ resid) / df_resid\n",
    "    XtX_inv = np.linalg.pinv(X_design.T @ X_design)\n",
    "    se = np.sqrt(np.diag(XtX_inv) * sigma2)\n",
    "    t_stats = np.divide(beta, se, out=np.full_like(beta, np.nan), where=se > 0)\n",
    "    p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)\n",
    "\n",
    "    # Recover the positions for post and treated_ai (they are the last two columns via 'remainder=passthrough')\n",
    "    # Column order in transformed matrix: [onehots..., post, treated_ai]\n",
    "    # So positions are: intercept=0, then all onehots, then post, then treated_ai\n",
    "    k = X_design.shape[1]\n",
    "    post_idx = k-2  # second last\n",
    "    treat_idx = k-1 # last\n",
    "    return {\n",
    "        \"y\": ycol,\n",
    "        \"alpha\": alpha,\n",
    "        \"coef_post\": float(beta[post_idx]),\n",
    "        \"pvalue_post\": float(np.round(p_vals[post_idx], 4)),\n",
    "        \"post_significant\": \"yes\" if p_vals[post_idx] < alpha else \"no\",\n",
    "        \"coef_treated_ai (ATT)\": float(beta[treat_idx]),\n",
    "        \"pvalue_treated_ai (ATT)\": float(np.round(p_vals[treat_idx], 4)),\n",
    "        \"treated_ai_significant\": \"yes\" if p_vals[treat_idx] < alpha else \"no\",\n",
    "    }\n",
    "\n",
    "did_time = did_regression(\"time_spent_search_sec\")\n",
    "did_rev  = did_regression(\"google_revenue_usd\")\n",
    "did_df = pd.DataFrame([did_time, did_rev])\n",
    "\n",
    "did_df\n",
    "\n",
    "# Interpretation: That means treated_ai is already the interaction term post * treated_group. There’s no need to create it  \n",
    "#   again—there simply aren’t any observations with treated_ai = 1 before the policy kicks in. The regression \n",
    "#   therefore has:                                                                                            \n",
    "                                                                                                            \n",
    "#   - Intercept (baseline non-AI in pre),                                                                     \n",
    "#   - post (time shift hitting both groups), and                                                              \n",
    "#   - treated_ai (effect that only appears for treated units in the post period).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7432779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary:\n",
      "                     metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      value\n",
      "             DiD_rev_ATT_ai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.040565\n",
      "          DiD_rev_coef_post                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.00417\n",
      "            DiD_time_ATT_ai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -4.801788\n",
      "         DiD_time_coef_post                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.77835\n",
      " ITE_rev_significant_groups [{'category': 'Entertainment', 'query_len_bucket': 'long(≥7)'}, {'category': 'Entertainment', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Entertainment', 'query_len_bucket': 'short(≤3)'}, {'category': 'Finance', 'query_len_bucket': 'long(≥7)'}, {'category': 'Finance', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'long(≥7)'}, {'category': 'Health', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'short(≤3)'}, {'category': 'HowTo', 'query_len_bucket': 'long(≥7)'}, {'category': 'HowTo', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Restaurants', 'query_len_bucket': 'long(≥7)'}, {'category': 'Restaurants', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Science', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Shopping', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Society', 'query_len_bucket': 'long(≥7)'}, {'category': 'Society', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Tech', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Travel', 'query_len_bucket': 'long(≥7)'}, {'category': 'Travel', 'query_len_bucket': 'medium(4–6)'}]\n",
      " ITE_rev_significant_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         19\n",
      "       ITE_rev_total_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         20\n",
      "ITE_time_significant_groups     [{'category': 'Entertainment', 'query_len_bucket': 'long(≥7)'}, {'category': 'Entertainment', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Entertainment', 'query_len_bucket': 'short(≤3)'}, {'category': 'Finance', 'query_len_bucket': 'long(≥7)'}, {'category': 'Finance', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'long(≥7)'}, {'category': 'Health', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'short(≤3)'}, {'category': 'HowTo', 'query_len_bucket': 'long(≥7)'}, {'category': 'HowTo', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Restaurants', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Science', 'query_len_bucket': 'long(≥7)'}, {'category': 'Science', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Shopping', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Society', 'query_len_bucket': 'long(≥7)'}, {'category': 'Society', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Tech', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Travel', 'query_len_bucket': 'long(≥7)'}, {'category': 'Travel', 'query_len_bucket': 'medium(4–6)'}]\n",
      "ITE_time_significant_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         19\n",
      "      ITE_time_total_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         20\n",
      "     T-learner R2 (revenue)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'r2_treated': 0.12158287894727948, 'r2_control': 0.152623255190512}\n",
      "        T-learner R2 (time)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'r2_treated': 0.08711715440986167, 'r2_control': 0.09074920185046442}\n",
      "Results summary:\n",
      "                     metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      value\n",
      "             DiD_rev_ATT_ai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.040565\n",
      "          DiD_rev_coef_post                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.00417\n",
      "            DiD_time_ATT_ai                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -4.801788\n",
      "         DiD_time_coef_post                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.77835\n",
      " ITE_rev_significant_groups [{'category': 'Entertainment', 'query_len_bucket': 'long(≥7)'}, {'category': 'Entertainment', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Entertainment', 'query_len_bucket': 'short(≤3)'}, {'category': 'Finance', 'query_len_bucket': 'long(≥7)'}, {'category': 'Finance', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'long(≥7)'}, {'category': 'Health', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'short(≤3)'}, {'category': 'HowTo', 'query_len_bucket': 'long(≥7)'}, {'category': 'HowTo', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Restaurants', 'query_len_bucket': 'long(≥7)'}, {'category': 'Restaurants', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Science', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Shopping', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Society', 'query_len_bucket': 'long(≥7)'}, {'category': 'Society', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Tech', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Travel', 'query_len_bucket': 'long(≥7)'}, {'category': 'Travel', 'query_len_bucket': 'medium(4–6)'}]\n",
      " ITE_rev_significant_groups [{'category': 'Entertainment', 'query_len_bucket': 'long(≥7)'}, {'category': 'Entertainment', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Entertainment', 'query_len_bucket': 'short(≤3)'}, {'category': 'Finance', 'query_len_bucket': 'long(≥7)'}, {'category': 'Finance', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'long(≥7)'}, {'category': 'Health', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'short(≤3)'}, {'category': 'HowTo', 'query_len_bucket': 'long(≥7)'}, {'category': 'HowTo', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Restaurants', 'query_len_bucket': 'long(≥7)'}, {'category': 'Restaurants', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Science', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Shopping', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Society', 'query_len_bucket': 'long(≥7)'}, {'category': 'Society', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Tech', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Travel', 'query_len_bucket': 'long(≥7)'}, {'category': 'Travel', 'query_len_bucket': 'medium(4–6)'}]\n",
      " ITE_rev_significant_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         19\n",
      "       ITE_rev_total_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         20\n",
      "       ITE_rev_total_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         20\n",
      "ITE_time_significant_groups     [{'category': 'Entertainment', 'query_len_bucket': 'long(≥7)'}, {'category': 'Entertainment', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Entertainment', 'query_len_bucket': 'short(≤3)'}, {'category': 'Finance', 'query_len_bucket': 'long(≥7)'}, {'category': 'Finance', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'long(≥7)'}, {'category': 'Health', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Health', 'query_len_bucket': 'short(≤3)'}, {'category': 'HowTo', 'query_len_bucket': 'long(≥7)'}, {'category': 'HowTo', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Restaurants', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Science', 'query_len_bucket': 'long(≥7)'}, {'category': 'Science', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Shopping', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Society', 'query_len_bucket': 'long(≥7)'}, {'category': 'Society', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Tech', 'query_len_bucket': 'medium(4–6)'}, {'category': 'Travel', 'query_len_bucket': 'long(≥7)'}, {'category': 'Travel', 'query_len_bucket': 'medium(4–6)'}]\n",
      "ITE_time_significant_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         19\n",
      "      ITE_time_total_slices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         20\n",
      "     T-learner R2 (revenue)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'r2_treated': 0.12158287894727948, 'r2_control': 0.152623255190512}\n",
      "        T-learner R2 (time)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'r2_treated': 0.08711715440986167, 'r2_control': 0.09074920185046442}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DiD_rev_ATT_ai</td>\n",
       "      <td>-0.040565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DiD_rev_coef_post</td>\n",
       "      <td>-0.00417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DiD_time_ATT_ai</td>\n",
       "      <td>-4.801788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DiD_time_coef_post</td>\n",
       "      <td>-0.77835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ITE_rev_significant_groups</td>\n",
       "      <td>[{'category': 'Entertainment', 'query_len_buck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ITE_rev_significant_groups</td>\n",
       "      <td>[{'category': 'Entertainment', 'query_len_buck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ITE_rev_significant_slices</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ITE_rev_total_slices</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ITE_rev_total_slices</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ITE_time_significant_groups</td>\n",
       "      <td>[{'category': 'Entertainment', 'query_len_buck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ITE_time_significant_slices</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ITE_time_total_slices</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T-learner R2 (revenue)</td>\n",
       "      <td>{'r2_treated': 0.12158287894727948, 'r2_contro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T-learner R2 (time)</td>\n",
       "      <td>{'r2_treated': 0.08711715440986167, 'r2_contro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  \\\n",
       "0                DiD_rev_ATT_ai   \n",
       "1             DiD_rev_coef_post   \n",
       "2               DiD_time_ATT_ai   \n",
       "3            DiD_time_coef_post   \n",
       "4    ITE_rev_significant_groups   \n",
       "5    ITE_rev_significant_groups   \n",
       "6    ITE_rev_significant_slices   \n",
       "7          ITE_rev_total_slices   \n",
       "8          ITE_rev_total_slices   \n",
       "9   ITE_time_significant_groups   \n",
       "10  ITE_time_significant_slices   \n",
       "11        ITE_time_total_slices   \n",
       "12       T-learner R2 (revenue)   \n",
       "13          T-learner R2 (time)   \n",
       "\n",
       "                                                value  \n",
       "0                                           -0.040565  \n",
       "1                                            -0.00417  \n",
       "2                                           -4.801788  \n",
       "3                                            -0.77835  \n",
       "4   [{'category': 'Entertainment', 'query_len_buck...  \n",
       "5   [{'category': 'Entertainment', 'query_len_buck...  \n",
       "6                                                  19  \n",
       "7                                                  20  \n",
       "8                                                  20  \n",
       "9   [{'category': 'Entertainment', 'query_len_buck...  \n",
       "10                                                 19  \n",
       "11                                                 20  \n",
       "12  {'r2_treated': 0.12158287894727948, 'r2_contro...  \n",
       "13  {'r2_treated': 0.08711715440986167, 'r2_contro...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- (2) Slice effects by category and query length ----------\n",
    "# We'll compute post-period difference between AI vs non-AI for each slice (ATT within post).\n",
    "# • Slice Effects\n",
    "                                                                                                            \n",
    "#   - post_subset = df[df[\"post\"]==1] filters to the post-period.\n",
    "#   - Group by the requested slicer columns plus ai_overview (groupby(group_cols + [\"ai_overview\"])) and take \n",
    "#     the mean of ycol.                                                                                       \n",
    "#   - unstack(\"ai_overview\") pivots so each row has separate columns for AI/Non-AI mean.                      \n",
    "#   - Renames those columns, computes ATT (post: ai - non_ai) as the difference, and resets the index for a   \n",
    "#     tidy DataFrame.                                                                                         \n",
    "#   - slice_cat_* / slice_len_* just run that helper for time and revenue outcomes split by category or query-    length bucket.                                                                                          \n",
    "                                                                                                            \n",
    "#   So you get post-period, within-slice average treatment effects (AI − non-AI) without any modeling—just    \n",
    "#   direct differences.          \n",
    "\n",
    "def slice_effects(group_cols, ycol):\n",
    "    post_subset = df[df[\"post\"]==1]\n",
    "    g = post_subset.groupby(group_cols + [\"ai_overview\"])[ycol].mean().unstack(\"ai_overview\")\n",
    "    g = g.rename(columns={False:\"mean_non_ai\", True:\"mean_ai\"})\n",
    "    g[\"ATT (post: ai - non_ai)\"] = g[\"mean_ai\"] - g[\"mean_non_ai\"]\n",
    "    return g.reset_index()\n",
    "\n",
    "slice_cat_time = slice_effects([\"category\"], \"time_spent_search_sec\")\n",
    "slice_len_time = slice_effects([\"query_len_bucket\"], \"time_spent_search_sec\")\n",
    "slice_cat_rev  = slice_effects([\"category\"], \"google_revenue_usd\")\n",
    "slice_len_rev  = slice_effects([\"query_len_bucket\"], \"google_revenue_usd\")\n",
    "\n",
    "# ---------- (3) Simple treatment-effect model (T-learner proxy for causal forest) ----------\n",
    "# Features (X): category, long_tail, query_len_words, weekday, ads_viewed (pre-exposure proxy), etc.\n",
    "# We exclude ai_overview from features for the control model, and include standard context.\n",
    "model_features = [\"category\",\"long_tail\",\"query_len_words\",\"cpc_usd\",\"ads_viewed\",\"post\"]\n",
    "\n",
    "# Train two models for y1 (treated) and y0 (control) on post period to avoid distribution shift across time\n",
    "post_data = df[df[\"post\"]==1].copy()\n",
    "\n",
    "#   T-Learner Block                                                                                           \n",
    "                                                                                                            \n",
    "#   - Uses only post-period rows (post_data) to avoid time drift.                                             \n",
    "#   - Splits into treated (ai_overview==1) and control (==0).                                                 \n",
    "#   - Builds two identical pipelines: one-hot encoding for category, then a RandomForestRegressor (200 trees, \n",
    "#     parallel).                                                                                              \n",
    "#   - Fits one model (pipe_t) on treated data and another (pipe_c) on control data.                           \n",
    "#   - Predicts both potential outcomes (mu1 if treated, mu0 if control) for every post-period observation by  \n",
    "#     feeding the same features through both models.                                                          \n",
    "#   - Individual treatment effect estimate ITE_hat is mu1 - mu0.                                              \n",
    "#   - Evaluates in-sample fit via R² on treated and control subsets for a sanity check.                       \n",
    "#   - Returns a DataFrame with the actual outcome, the two predicted potential outcomes, the individual       \n",
    "#     effect, and some key descriptors (category, query length).\n",
    "#   - summarize_ite then groups those ITEs to report mean/median/percentiles by category and query-length     \n",
    "#     bucket.\n",
    "\n",
    "#   Finally, display_dataframe_to_user shows all the tables (DiD summary, slice ATT tables, T-learner\n",
    "#   summaries), and results collects a few headline numbers.\n",
    "# ---------- (3) Simple treatment-effect model (T-learner proxy for causal forest) ----------             \n",
    "  # Features (X): category, long_tail, query_len_words, weekday, ads_viewed (pre-exposure proxy), etc.      \n",
    "model_features = [\"category\",\"long_tail\",\"query_len_words\",\"cpc_usd\",\"ads_viewed\",\"post\"]                 \n",
    "                                                                                                            \n",
    "  # Train two models for y1 (treated) and y0 (control) on post period to avoid distribution shift across    \n",
    "#  time                                                                                                      \n",
    "post_data = df[df[\"post\"]==1].copy()                                                                      \n",
    "                                                                                                            \n",
    "def build_t_learner(ycol):                                                                                \n",
    "      treat = post_data[post_data[\"ai_overview\"]==1].copy()\n",
    "      ctrl  = post_data[post_data[\"ai_overview\"]==0].copy()                                                 \n",
    "      preproc = ColumnTransformer([                                                                         \n",
    "          (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"category\"])                                     \n",
    "      ], remainder=\"passthrough\")                                                                           \n",
    "      rf_t = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)                            \n",
    "      rf_c = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)                            \n",
    "      pipe_t = Pipeline([(\"prep\", preproc), (\"rf\", rf_t)])                                                  \n",
    "      pipe_c = Pipeline([(\"prep\", preproc), (\"rf\", rf_c)])                                                  \n",
    "      X_t, y_t = treat[model_features], treat[ycol]                                                         \n",
    "      X_c, y_c = ctrl[model_features],  ctrl[ycol]                                                          \n",
    "      pipe_t.fit(X_t, y_t)\n",
    "      pipe_c.fit(X_c, y_c)\n",
    "      X_all = post_data[model_features]                                                                     \n",
    "      mu1 = pipe_t.predict(X_all)\n",
    "      mu0 = pipe_c.predict(X_all)                                                                           \n",
    "      ite = mu1 - mu0\n",
    "      r2_t = r2_score(y_t, pipe_t.predict(X_t))                                                             \n",
    "      r2_c = r2_score(y_c, pipe_c.predict(X_c))                                                             \n",
    "      out = post_data[[\"category\",\"query_len_bucket\",\"ai_overview\"]].copy()                                 \n",
    "      out[\"y_actual\"] = post_data[ycol].values                                                              \n",
    "      out[\"mu1_hat\"] = mu1                                                                                  \n",
    "      out[\"mu0_hat\"] = mu0                                                                                  \n",
    "      out[\"ITE_hat\"] = ite                                                                                  \n",
    "      return out, {\"r2_treated\": r2_t, \"r2_control\": r2_c}                                                  \n",
    "                                                                                                            \n",
    "t_time_df, t_time_metrics = build_t_learner(\"time_spent_search_sec\")                                      \n",
    "t_rev_df,  t_rev_metrics  = build_t_learner(\"google_revenue_usd\")                                         \n",
    "                                                                                                            \n",
    "  # Aggregate ITEs for interpretability                                                                     \n",
    "def summarize_ite(ite_df, alpha=0.05):                                                                    \n",
    "      group_cols = [\"category\", \"query_len_bucket\"]                                                         \n",
    "      summaries = []                                                                                        \n",
    "      for keys, grp in ite_df.groupby(group_cols):                                                          \n",
    "          values = grp[\"ITE_hat\"].dropna()                                                                  \n",
    "          n_obs = int(values.shape[0])                                                                      \n",
    "          mean_ite = values.mean() if n_obs else float(\"nan\")                                               \n",
    "          median_ite = values.median() if n_obs else float(\"nan\")                                           \n",
    "          p25_ite = float(np.percentile(values, 25)) if n_obs else float(\"nan\")                             \n",
    "          p75_ite = float(np.percentile(values, 75)) if n_obs else float(\"nan\")                             \n",
    "          if n_obs > 1:                                                                                     \n",
    "              std_ite = float(values.std(ddof=1))                                                           \n",
    "          elif n_obs == 1:                                                                                  \n",
    "              std_ite = 0.0                                                                                 \n",
    "          else:                                                                                             \n",
    "              std_ite = float(\"nan\")                                                                        \n",
    "          se_ite = std_ite / np.sqrt(n_obs) if n_obs > 0 else float(\"nan\")\n",
    "          if n_obs > 1 and se_ite != 0:                                                                     \n",
    "              t_stat = mean_ite / se_ite                                                                    \n",
    "              p_value = 2 * stats.t.sf(np.abs(t_stat), df=n_obs - 1)                                        \n",
    "          else:                                                                                             \n",
    "              t_stat = float(\"nan\")                                                                         \n",
    "              p_value = float(\"nan\")                                                                        \n",
    "          summaries.append({                                                                                \n",
    "              \"category\": keys[0],\n",
    "              \"query_len_bucket\": keys[1],                                                                  \n",
    "              \"mean_ITE\": mean_ite,                                                                         \n",
    "              \"median_ITE\": median_ite,                                                                     \n",
    "              \"p25_ITE\": p25_ite,                                                                           \n",
    "              \"p75_ITE\": p75_ite,                                                                           \n",
    "              \"std_ITE\": std_ite,\n",
    "              \"se_ITE\": se_ite,                                                                             \n",
    "              \"t_stat\": t_stat,                                                                             \n",
    "              \"p_value\": p_value,                                                                           \n",
    "              \"significant\": \"yes\" if (not pd.isna(p_value) and p_value < alpha) else \"no\",                 \n",
    "              \"n_obs\": n_obs,                                                                               \n",
    "          })                                                                                                \n",
    "      summary_df = pd.DataFrame(summaries)                                                                  \n",
    "      if not summary_df.empty:                                                                              \n",
    "          summary_df = summary_df.sort_values(group_cols).reset_index(drop=True)                            \n",
    "          for col in [\"mean_ITE\", \"median_ITE\", \"p25_ITE\", \"p75_ITE\", \"std_ITE\", \"se_ITE\", \"t_stat\",        \n",
    "  \"p_value\"]:     \n",
    "              summary_df[col] = summary_df[col].round(4)                                                    \n",
    "      return summary_df                                                                                     \n",
    "                                                                                                            \n",
    "ite_time_summary = summarize_ite(t_time_df)                                                               \n",
    "ite_rev_summary  = summarize_ite(t_rev_df)                                                                \n",
    "                                                                                                            \n",
    "  # Also provide quick textual summaries in a Pandas table                                                  \n",
    "results_summary = []                                                                                      \n",
    "results_summary.append({\"metric\": \"DiD_time_coef_post\", \"value\": float(did_time[\"coef_post\"])})           \n",
    "results_summary.append({\"metric\": \"DiD_time_ATT_ai\", \"value\": float(did_time[\"coef_treated_ai (ATT)\"])})  \n",
    "results_summary.append({\"metric\": \"DiD_rev_coef_post\", \"value\": float(did_rev[\"coef_post\"])})             \n",
    "results_summary.append({\"metric\": \"DiD_rev_ATT_ai\", \"value\": float(did_rev[\"coef_treated_ai (ATT)\"])})    \n",
    "results_summary.append({\"metric\": \"T-learner R2 (time)\", \"value\": t_time_metrics})                        \n",
    "results_summary.append({\"metric\": \"T-learner R2 (revenue)\", \"value\": t_rev_metrics})                      \n",
    "results_summary.append({\"metric\": \"ITE_time_significant_slices\",                                          \n",
    "                          \"value\": int((ite_time_summary[\"significant\"] == \"yes\").sum())})\n",
    "results_summary.append({\"metric\": \"ITE_time_total_slices\",                                                \n",
    "                          \"value\": int(len(ite_time_summary))})                                             \n",
    "results_summary.append({\"metric\": \"ITE_time_significant_groups\",                                          \n",
    "                          \"value\": (                                                                        \n",
    "                              ite_time_summary[ite_time_summary[\"significant\"] == \"yes\"]                    \n",
    "                              [[\"category\", \"query_len_bucket\"]].to_dict(\"records\")                         \n",
    "                          )})                                                                               \n",
    "results_summary.append({\"metric\": \"ITE_rev_significant_slices\",                                           \n",
    "                          \"value\": int((ite_rev_summary[\"significant\"] == \"yes\").sum())})                   \n",
    "results_summary.append({\"metric\": \"ITE_rev_total_slices\",                                                 \n",
    "                          \"value\": int(len(ite_rev_summary))})                                              \n",
    "results_summary.append({\"metric\": \"ITE_rev_significant_groups\",\n",
    "                          \"value\": (                                                                        \n",
    "                              ite_rev_summary[ite_rev_summary[\"significant\"] == \"yes\"]\n",
    "                              [[\"category\", \"query_len_bucket\"]].to_dict(\"records\")                         \n",
    "                          )})                                                                               \n",
    "                                                                                                            \n",
    "results_df = pd.DataFrame(results_summary).sort_values(\"metric\").reset_index(drop=True)                   \n",
    "                                                                                                            \n",
    "print(\"Results summary:\")                                                                                 \n",
    "print(results_df.to_string(index=False))                                                                  \n",
    "results_df                                                                                                \n",
    "results_summary.append({\"metric\": \"ITE_rev_total_slices\",\n",
    "                          \"value\": int(len(ite_rev_summary))})\n",
    "results_summary.append({\"metric\": \"ITE_rev_significant_groups\",\n",
    "                          \"value\": (\n",
    "                              ite_rev_summary[ite_rev_summary[\"significant\"] == \"yes\"]\n",
    "                              [[\"category\", \"query_len_bucket\"]].to_dict(\"records\")\n",
    "                          )})\n",
    "\n",
    "results_df = pd.DataFrame(results_summary).sort_values(\"metric\").reset_index(drop=True)\n",
    "\n",
    "print(\"Results summary:\")\n",
    "print(results_df.to_string(index=False))\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d21c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Summary Explained\n",
    "                                                                                                            \n",
    "#   - DiD_rev_ATT_ai  -0.040565: After controlling for group and time effects, AI overviews cut average post- \n",
    "#     period revenue per query by about $0.04 relative to the non-AI trend (ATT).           \n",
    "#                   \n",
    "#   - DiD_rev_coef_post  -0.00417: Even without AI exposure, revenue dropped by roughly $0.004 between pre and\n",
    "#     post periods— a common time shock affecting everyone.                                                   \n",
    "\n",
    "#   - DiD_time_ATT_ai  -4.801788: AI overviews shortened time-on-search by ~4.8 seconds versus the non-AI     \n",
    "#     counterfactual— a large DiD effect.                                                                     \n",
    "\n",
    "#   - DiD_time_coef_post  -0.77835: Across the board, search sessions became about 0.78 seconds shorter post  \n",
    "#     period, regardless of AI exposure.                                                                      \n",
    "\n",
    "#   - ITE_rev_significant_groups …: Outlines each (category, query_len_bucket) slice where the T-learner      \n",
    "#     estimated revenue uplift is statistically significant (19 entries, e.g., Entertainment/short).          \n",
    "\n",
    "#   - ITE_rev_significant_slices  19: Confirms the count of significant revenue slices—19 out of the total    \n",
    "#     evaluated.                                                                                              \n",
    "\n",
    "#   - ITE_rev_total_slices  20: Indicates there were 20 category×query-length slices with enough data; 19     \n",
    "#     showed significant revenue effects (the repetition in the table is just due to the verbose summary).    \n",
    "\n",
    "#   - ITE_time_significant_groups …: Lists the slices with significant time-on-search effects (again 19       \n",
    "#     entries, parallel to the revenue list).                                                                 \n",
    "\n",
    "#   - ITE_time_significant_slices  19: Shows 19 slices had significant time effects.\n",
    "\n",
    "#   - ITE_time_total_slices  20: Out of 20 slices checked, 19 were significant for time.\n",
    "\n",
    "#   - T-learner R2 (revenue) {r2_treated≈0.12, r2_control≈0.15}: The random-forest models explain about 12%   \n",
    "#     and 15% of revenue variance for the treated and control groups respectively—moderate explanatory power. \n",
    "\n",
    "#   - T-learner R2 (time) {r2_treated≈0.09, r2_control≈0.09}: The time-on-search models explain ~9% of        \n",
    "#     variance; predictive strength is low, so individual effect estimates should be treated with caution.    \n",
    "#     counterfactual— a large DiD effect.\n",
    "\n",
    "# Interpretation Notes:\n",
    "#   Revenue: the models explain ~12% of variation for AI queries and ~15% for non-AI. That’s a modest     \n",
    "    # but useful signal—features like category, CPC, and ads viewed do capture some revenue dynamics, so the  \n",
    "    # slice-level treatment effects you’re seeing likely reflect real differences, even if there’s plenty of  \n",
    "    # noise left unexplained.\n",
    "# Time on search: the models explain only ~9% of variance for either group. User-level dwell time is    \n",
    "    # harder to predict from the available features, so the individualized ITE estimates are less reliable.   \n",
    "    # Treat the aggregate “significant slices” as directional guidance rather than precise numbers, and       \n",
    "    # consider collecting richer behavioral features if you need stronger causal granularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82591f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ffbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1f37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de12ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d234a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb786a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a622e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
