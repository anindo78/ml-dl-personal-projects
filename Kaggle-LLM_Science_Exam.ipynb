{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d097a44-7732-4732-84d9-29d59874ff1d",
   "metadata": {},
   "source": [
    "### https://www.kaggle.com/competitions/kaggle-llm-science-exam/overview\n",
    "\n",
    "#### Context\n",
    "As the scope of large language model capabilities expands, a growing area of research is using LLMs to characterize themselves. Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that LLMs can be used to create more challenging tasks to test ever more powerful models.\n",
    "\n",
    "At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware. The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits.\n",
    "\n",
    "The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.\n",
    "\n",
    "Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters. If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves.\n",
    "\n",
    "#### I am using https://www.kaggle.com/code/wlifferth/starter-notebook-ranked-predictions-with-bert as a starter notebook. All due to credit to the author William Lifferth for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cd320-92f8-48b7-a489-2161a3e1678f",
   "metadata": {},
   "source": [
    "#### LLM Science Exam\n",
    "This starter notebook walks through a basic example of using BERT to rank the answers to each question. We'll finetune BERT on the 200 public questions, then use the AutoModelForMultipleChoice class to generate probabilities that each option correctly answers the prompt, and finally we'll turn those predictions into a MAP@3-formatted prediction like A B C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1610665e-8756-4781-8850-d4281fc400b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\etc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6164d929-3d87-45f9-85dc-83bf7f25f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c36b84-bf45-4f52-9d82-10c52d7ebf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  \\\n",
       "0   0  Which of the following statements accurately d...   \n",
       "1   1  Which of the following is an accurate definiti...   \n",
       "2   2  Which of the following statements accurately d...   \n",
       "3   3  What is the significance of regularization in ...   \n",
       "4   4  Which of the following statements accurately d...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  MOND is a theory that reduces the observed mis...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol was reconstructed as a fe...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  MOND is a theory that increases the discrepanc...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol is a representation of th...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  MOND is a theory that explains the missing bar...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol is a representation of a ...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  MOND is a theory that reduces the discrepancy ...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol represents three interloc...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   E answer  \n",
       "0  MOND is a theory that eliminates the observed ...      D  \n",
       "1  Dynamic scaling refers to the evolution of sel...      A  \n",
       "2  The triskeles symbol is a representation of th...      A  \n",
       "3  Regularizing the mass-energy of an electron wi...      C  \n",
       "4  The angular spacing of features in the diffrac...      D  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import the public training set and take a look\n",
    "\n",
    "train_df = pd.read_csv('kaggle-llm-science-exam/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998b4126-3739-4bc3-b981-8b8f4113551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience we'll turn our pandas Dataframe into a Dataset\n",
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306b0be5-2dc9-4117-81e7-40ec6344e68e",
   "metadata": {},
   "source": [
    "### What is Autotokenizer\n",
    "\n",
    "In 🤗 Transformers, AutoTokenizer is a factory that picks the right tokenizer class + vocab for a model name. You just give the checkpoint ID, and it returns the correct tokenizer (WordPiece for BERT, SentencePiece for ALBERT, BPE for RoBERTa, etc.). This ensures you don’t mix the wrong tokenizer with a model, which would hurt performance.\n",
    "\n",
    "The “best” one is the one paired with the model you’re using (same pretraining, same vocab). Within BERT-style models, pick based on language, casing, and domain.\n",
    "\n",
    "let's use **bert-base-cased** (WordPiece): better when casing matters (e.g., NER, proper nouns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9293789-a123-4fd0-aea6-3eb7c6c3d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# The path of the model checkpoint we want to use\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1be74c3-2a38-4cf1-8016-62fc3c0990e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b676c96-0c3d-4ffa-ac0c-e89441517c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d730aed33748239eded7a1d2c2301b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preproccess the data\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f86b634-ada5-4c7b-95c8-bc5fcddd0b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [0],\n",
       " 'input_ids': [[[101,\n",
       "    2029,\n",
       "    1997,\n",
       "    1996,\n",
       "    2206,\n",
       "    8635,\n",
       "    14125,\n",
       "    5577,\n",
       "    1996,\n",
       "    4254,\n",
       "    1997,\n",
       "    6310,\n",
       "    8446,\n",
       "    2937,\n",
       "    10949,\n",
       "    1006,\n",
       "    12256,\n",
       "    2094,\n",
       "    1007,\n",
       "    2006,\n",
       "    1996,\n",
       "    5159,\n",
       "    1000,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1000,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1029,\n",
       "    102,\n",
       "    12256,\n",
       "    2094,\n",
       "    2003,\n",
       "    1037,\n",
       "    3399,\n",
       "    2008,\n",
       "    13416,\n",
       "    1996,\n",
       "    5159,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    2011,\n",
       "    2695,\n",
       "    10924,\n",
       "    1996,\n",
       "    4598,\n",
       "    1997,\n",
       "    1037,\n",
       "    2047,\n",
       "    2433,\n",
       "    1997,\n",
       "    3043,\n",
       "    2170,\n",
       "    1000,\n",
       "    18001,\n",
       "    2601,\n",
       "    3043,\n",
       "    1012,\n",
       "    1000,\n",
       "    102],\n",
       "   [101,\n",
       "    2029,\n",
       "    1997,\n",
       "    1996,\n",
       "    2206,\n",
       "    8635,\n",
       "    14125,\n",
       "    5577,\n",
       "    1996,\n",
       "    4254,\n",
       "    1997,\n",
       "    6310,\n",
       "    8446,\n",
       "    2937,\n",
       "    10949,\n",
       "    1006,\n",
       "    12256,\n",
       "    2094,\n",
       "    1007,\n",
       "    2006,\n",
       "    1996,\n",
       "    5159,\n",
       "    1000,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1000,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1029,\n",
       "    102,\n",
       "    12256,\n",
       "    2094,\n",
       "    2003,\n",
       "    1037,\n",
       "    3399,\n",
       "    2008,\n",
       "    7457,\n",
       "    1996,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    2090,\n",
       "    1996,\n",
       "    5159,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1998,\n",
       "    1996,\n",
       "    7594,\n",
       "    10146,\n",
       "    4487,\n",
       "    17668,\n",
       "    27466,\n",
       "    2013,\n",
       "    1037,\n",
       "    5387,\n",
       "    1997,\n",
       "    2105,\n",
       "    2184,\n",
       "    2000,\n",
       "    1037,\n",
       "    5387,\n",
       "    1997,\n",
       "    2055,\n",
       "    2322,\n",
       "    1012,\n",
       "    102],\n",
       "   [101,\n",
       "    2029,\n",
       "    1997,\n",
       "    1996,\n",
       "    2206,\n",
       "    8635,\n",
       "    14125,\n",
       "    5577,\n",
       "    1996,\n",
       "    4254,\n",
       "    1997,\n",
       "    6310,\n",
       "    8446,\n",
       "    2937,\n",
       "    10949,\n",
       "    1006,\n",
       "    12256,\n",
       "    2094,\n",
       "    1007,\n",
       "    2006,\n",
       "    1996,\n",
       "    5159,\n",
       "    1000,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1000,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1029,\n",
       "    102,\n",
       "    12256,\n",
       "    2094,\n",
       "    2003,\n",
       "    1037,\n",
       "    3399,\n",
       "    2008,\n",
       "    7607,\n",
       "    1996,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    2008,\n",
       "    2001,\n",
       "    3130,\n",
       "    2641,\n",
       "    2601,\n",
       "    3043,\n",
       "    2011,\n",
       "    14313,\n",
       "    2008,\n",
       "    1996,\n",
       "    3742,\n",
       "    2003,\n",
       "    1999,\n",
       "    1996,\n",
       "    2433,\n",
       "    1997,\n",
       "    11265,\n",
       "    4904,\n",
       "    17815,\n",
       "    2015,\n",
       "    1998,\n",
       "    22260,\n",
       "    8496,\n",
       "    1012,\n",
       "    102],\n",
       "   [101,\n",
       "    2029,\n",
       "    1997,\n",
       "    1996,\n",
       "    2206,\n",
       "    8635,\n",
       "    14125,\n",
       "    5577,\n",
       "    1996,\n",
       "    4254,\n",
       "    1997,\n",
       "    6310,\n",
       "    8446,\n",
       "    2937,\n",
       "    10949,\n",
       "    1006,\n",
       "    12256,\n",
       "    2094,\n",
       "    1007,\n",
       "    2006,\n",
       "    1996,\n",
       "    5159,\n",
       "    1000,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1000,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1029,\n",
       "    102,\n",
       "    12256,\n",
       "    2094,\n",
       "    2003,\n",
       "    1037,\n",
       "    3399,\n",
       "    2008,\n",
       "    13416,\n",
       "    1996,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    2090,\n",
       "    1996,\n",
       "    5159,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1998,\n",
       "    1996,\n",
       "    7594,\n",
       "    10146,\n",
       "    4487,\n",
       "    17668,\n",
       "    27466,\n",
       "    2013,\n",
       "    1037,\n",
       "    5387,\n",
       "    1997,\n",
       "    2105,\n",
       "    2184,\n",
       "    2000,\n",
       "    1037,\n",
       "    5387,\n",
       "    1997,\n",
       "    2055,\n",
       "    1016,\n",
       "    1012,\n",
       "    102],\n",
       "   [101,\n",
       "    2029,\n",
       "    1997,\n",
       "    1996,\n",
       "    2206,\n",
       "    8635,\n",
       "    14125,\n",
       "    5577,\n",
       "    1996,\n",
       "    4254,\n",
       "    1997,\n",
       "    6310,\n",
       "    8446,\n",
       "    2937,\n",
       "    10949,\n",
       "    1006,\n",
       "    12256,\n",
       "    2094,\n",
       "    1007,\n",
       "    2006,\n",
       "    1996,\n",
       "    5159,\n",
       "    1000,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1000,\n",
       "    5860,\n",
       "    2890,\n",
       "    9739,\n",
       "    5666,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    1029,\n",
       "    102,\n",
       "    12256,\n",
       "    2094,\n",
       "    2003,\n",
       "    1037,\n",
       "    3399,\n",
       "    2008,\n",
       "    11027,\n",
       "    2015,\n",
       "    1996,\n",
       "    5159,\n",
       "    4394,\n",
       "    3347,\n",
       "    14001,\n",
       "    2594,\n",
       "    3742,\n",
       "    1999,\n",
       "    9088,\n",
       "    12906,\n",
       "    2011,\n",
       "    16625,\n",
       "    1037,\n",
       "    2047,\n",
       "    8045,\n",
       "    20219,\n",
       "    1997,\n",
       "    8992,\n",
       "    2008,\n",
       "    2515,\n",
       "    2025,\n",
       "    5478,\n",
       "    1996,\n",
       "    4598,\n",
       "    1997,\n",
       "    2601,\n",
       "    3043,\n",
       "    1012,\n",
       "    102]]],\n",
       " 'token_type_ids': [[[0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1]]],\n",
       " 'attention_mask': [[[1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1],\n",
       "   [1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1]]],\n",
       " 'label': [3]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ccca4-de8d-4c2e-96d2-587235b9e214",
   "metadata": {},
   "source": [
    "### We will be using AutoModelForMultipleChoice class\n",
    "\n",
    "#### What it does\n",
    "\n",
    "It wraps a pretrained encoder (like BERT, RoBERTa, DeBERTa, etc.) plus a classification head designed for multiple-choice input.\n",
    "\n",
    "The model expects:\n",
    "\n",
    "A batch of questions with several answer options each,\n",
    "\n",
    "It processes each option separately through the encoder,\n",
    "\n",
    "Then applies a classification layer to score each option,\n",
    "\n",
    "And outputs a logit per choice (higher = more likely correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018384af-5ad4-4c69-a440-63de0e1f3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n",
    "# will dynamically pad our questions at batch-time so we don't have to make every question the length\n",
    "# of our longest question.\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950ce993-05cf-4ca4-9b15-60c6d847bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebfba93b-b0ae-4d68-9a58-2a1bc8c542aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Now we'll instatiate the model that we'll finetune on our public dataset, then use to\n",
    "# make prediction on the private dataset.\n",
    "\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce22829-b642-4522-941e-d0987ef59126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arguments here are selected to run quickly; feel free to play with them.\n",
    "model_dir = 'finetuned_bert'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cdbb78f-9b80-4088-b15c-fe39eb5123a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\AppData\\Local\\Temp\\ipykernel_10196\\2146174223.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Generally it's a bad idea to validate on your training set, but because our training set\n",
    "# for this problem is so small we're going to train on all our data.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcbead09-b020-47f4-8067-75aaba261e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 38:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.603954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.584196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.431856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=1.5702899169921876, metrics={'train_runtime': 2321.7422, 'train_samples_per_second': 0.258, 'train_steps_per_second': 0.065, 'total_flos': 129426866048760.0, 'train_loss': 1.5702899169921876, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training should take about a minute\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2d004b-d24a-4fc1-9a51-44aaeb780c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can actually make predictions on our questions\n",
    "predictions = trainer.predict(tokenized_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad4f6bb-d9f6-427e-add3-5a7117fee589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function gets the indices of the highest scoring answers for each row\n",
    "# and converts them back to our answer format (A, B, C, D, E)\n",
    "import numpy as np\n",
    "def predictions_to_map_output(predictions):\n",
    "    sorted_answer_indices = np.argsort(-predictions)\n",
    "    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n",
    "    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n",
    "    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b1e8458-51fc-4fb4-b4cc-505c481a3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D B C', 'A C D', 'A C B', 'C E A', 'D B E', 'C B E', 'A C D',\n",
       "       'A D B', 'C E D', 'A C B', 'B E C', 'A D B', 'C B A', 'D C E',\n",
       "       'B A E', 'C A D', 'E B C', 'A D B', 'A B D', 'E D A', 'B D C',\n",
       "       'B C D', 'C A B', 'C A B', 'E A B', 'E D A', 'A B C', 'D C B',\n",
       "       'E B A', 'C B D', 'B C A', 'E C D', 'D E B', 'A C E', 'E D B',\n",
       "       'A B D', 'E A D', 'A C E', 'E D A', 'A E C', 'E B D', 'B E A',\n",
       "       'B C D', 'D C E', 'A E B', 'A B C', 'B E C', 'C B A', 'B E A',\n",
       "       'C E D', 'B D E', 'E D A', 'C A B', 'A C D', 'B A C', 'C E A',\n",
       "       'C B D', 'C B A', 'D A C', 'B C A', 'B E D', 'B E C', 'B C E',\n",
       "       'C A E', 'A E C', 'E A D', 'C D A', 'B E C', 'A C E', 'D B A',\n",
       "       'D E B', 'A E D', 'A D B', 'B D A', 'D A E', 'D B E', 'B D C',\n",
       "       'B E C', 'C E B', 'E A C', 'C D E', 'A C E', 'C B D', 'B D A',\n",
       "       'A B C', 'B A D', 'A D B', 'B D E', 'E A C', 'D B A', 'D B A',\n",
       "       'B E C', 'B E C', 'E B A', 'E B D', 'C D B', 'C D B', 'E C A',\n",
       "       'C B A', 'D C A', 'D E B', 'B E A', 'C A D', 'E C B', 'E A D',\n",
       "       'D B E', 'A B D', 'D E A', 'A E C', 'C D B', 'A C E', 'A C D',\n",
       "       'A D E', 'B C A', 'D E A', 'E B D', 'D C B', 'C E B', 'D E A',\n",
       "       'B C A', 'A D E', 'C A E', 'A B E', 'A E B', 'B A E', 'C D E',\n",
       "       'B D E', 'C D B', 'E B C', 'E D C', 'B E A', 'C B A', 'E D C',\n",
       "       'A E C', 'E C A', 'E C B', 'E B C', 'D B A', 'A E B', 'E B C',\n",
       "       'C E A', 'E B A', 'A B D', 'C B E', 'C E A', 'B C D', 'D A C',\n",
       "       'C A B', 'B D E', 'B D A', 'B E A', 'A B D', 'C A B', 'D A C',\n",
       "       'D E A', 'D B A', 'D B A', 'C A E', 'D A E', 'A B D', 'B E C',\n",
       "       'E C D', 'A C E', 'B A E', 'C D E', 'C D E', 'C B A', 'D A C',\n",
       "       'C E D', 'A B D', 'C B E', 'E B A', 'D C E', 'C E D', 'D B C',\n",
       "       'E C A', 'A E D', 'D C B', 'D A C', 'C E B', 'B C A', 'C B E',\n",
       "       'A C E', 'E A D', 'C A E', 'A E B', 'A B E', 'A B D', 'D B C',\n",
       "       'E B C', 'D E A', 'D C A', 'B D E', 'C B E', 'D E C', 'C A E',\n",
       "       'B C E', 'B C A', 'B D C', 'E B A'], dtype='<U5')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's double check our output looks correct:\n",
    "predictions_to_map_output(predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bcfb182-fd81-432a-beab-e24642239138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  \\\n",
       "0   0  Which of the following statements accurately d...   \n",
       "1   1  Which of the following is an accurate definiti...   \n",
       "2   2  Which of the following statements accurately d...   \n",
       "3   3  What is the significance of regularization in ...   \n",
       "4   4  Which of the following statements accurately d...   \n",
       "\n",
       "                                                   A  \\\n",
       "0  MOND is a theory that reduces the observed mis...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol was reconstructed as a fe...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   B  \\\n",
       "0  MOND is a theory that increases the discrepanc...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol is a representation of th...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   C  \\\n",
       "0  MOND is a theory that explains the missing bar...   \n",
       "1  Dynamic scaling refers to the evolution of sel...   \n",
       "2  The triskeles symbol is a representation of a ...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   D  \\\n",
       "0  MOND is a theory that reduces the discrepancy ...   \n",
       "1  Dynamic scaling refers to the non-evolution of...   \n",
       "2  The triskeles symbol represents three interloc...   \n",
       "3  Regularizing the mass-energy of an electron wi...   \n",
       "4  The angular spacing of features in the diffrac...   \n",
       "\n",
       "                                                   E  \n",
       "0  MOND is a theory that eliminates the observed ...  \n",
       "1  Dynamic scaling refers to the evolution of sel...  \n",
       "2  The triskeles symbol is a representation of th...  \n",
       "3  Regularizing the mass-energy of an electron wi...  \n",
       "4  The angular spacing of features in the diffrac...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can load up our test set to use our model on!\n",
    "# The public test.csv isn't the real dataset (it's actually just a copy of train.csv without the answer column)\n",
    "# but it has the same format as the real test set, so using it is a good way to ensure our code will work when we submit.\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('kaggle-llm-science-exam/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44dc99a2-5f44-4961-b691-3dd4aefce2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db018407ec14d2198619c35ce0653fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# There are more verbose/elegant ways of doing this, but if we give our test set a random `answer` column\n",
    "# we can make predictions directly with our trainer.\n",
    "\n",
    "test_df['answer'] = 'A'\n",
    "\n",
    "# Other than that we'll preprocess it in the same way we preprocessed test.csv\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "tokenized_test_ds = test_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d0d68-f76a-4ba8-8819-0db448bd90cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anind\\OneDrive\\Anindo\\Python\\projects\\aiprojects\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/50 02:14 < 00:02, 0.36 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we'll generate our \"real\" predictions on the test set\n",
    "test_predictions = trainer.predict(tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89070a92-973c-47ae-9ba4-ef289ae14360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create our submission using the id column from test.csv\n",
    "submission_df = test_df[['id']]\n",
    "submission_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88725c93-380f-4dfd-930c-fafa268e7256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66528f-41a1-4aa6-a618-551fc8663f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5fb9b-f18e-4165-b09d-50aaa1bbddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8f65b-e39c-4161-9d05-a551ee4cc961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd5f9b-33a3-414f-84d7-ae28d6ca5018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
